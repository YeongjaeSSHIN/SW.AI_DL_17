{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5f8e8275525418fbc2e33455985afb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26e73300a97e403692be605aa596e798",
              "IPY_MODEL_5a16994df309473eae456867653dc986",
              "IPY_MODEL_d312644e1e4648e29b7216472752a4ef"
            ],
            "layout": "IPY_MODEL_3cd7f6f5d38347fea196accc99ebfea5"
          }
        },
        "26e73300a97e403692be605aa596e798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cab2ca4536344078de852b1f391e2a4",
            "placeholder": "​",
            "style": "IPY_MODEL_92b497d742a44c74acb26f2e0513c0ef",
            "value": "model.safetensors: 100%"
          }
        },
        "5a16994df309473eae456867653dc986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06a762004c36439cb69d383d59192232",
            "max": 800582904,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd2cf5d576994ec9b55e0149c0ee27ea",
            "value": 800582904
          }
        },
        "d312644e1e4648e29b7216472752a4ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_049ec5f756e74d2389fb270ea5ee8fdd",
            "placeholder": "​",
            "style": "IPY_MODEL_13a65424362149ec9adbe65c703d03fd",
            "value": " 801M/801M [00:03&lt;00:00, 272MB/s]"
          }
        },
        "3cd7f6f5d38347fea196accc99ebfea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cab2ca4536344078de852b1f391e2a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92b497d742a44c74acb26f2e0513c0ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06a762004c36439cb69d383d59192232": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd2cf5d576994ec9b55e0149c0ee27ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "049ec5f756e74d2389fb270ea5ee8fdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13a65424362149ec9adbe65c703d03fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbf1d3a716fe441794d68fa708957f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df59f27d0e8749c8880cace12b4ef95b",
              "IPY_MODEL_eebab466632b4e4bbce57a534753635c",
              "IPY_MODEL_0eecafd8acfe480782b7f19c76f606bc"
            ],
            "layout": "IPY_MODEL_025b95c8495b41b08b20efd3101a96d6"
          }
        },
        "df59f27d0e8749c8880cace12b4ef95b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c98a9526f0c44ea29549f6890fe6ff7a",
            "placeholder": "​",
            "style": "IPY_MODEL_f708de65fc184f6c8983dcb1b322ed8e",
            "value": "model.safetensors: 100%"
          }
        },
        "eebab466632b4e4bbce57a534753635c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6fb6b2b31b24254b205039aa2294a8a",
            "max": 800582904,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c8e935222cb4d94892a6ba71ed5856b",
            "value": 800582904
          }
        },
        "0eecafd8acfe480782b7f19c76f606bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0d2cbd49458484fafc1806146e5bbfe",
            "placeholder": "​",
            "style": "IPY_MODEL_e2b59e7cc1264ce8833992749ee60425",
            "value": " 801M/801M [00:03&lt;00:00, 275MB/s]"
          }
        },
        "025b95c8495b41b08b20efd3101a96d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c98a9526f0c44ea29549f6890fe6ff7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f708de65fc184f6c8983dcb1b322ed8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6fb6b2b31b24254b205039aa2294a8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c8e935222cb4d94892a6ba71ed5856b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0d2cbd49458484fafc1806146e5bbfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2b59e7cc1264ce8833992749ee60425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6413ce0457a424d815810d49646500a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfaede04cda04fd4b64ffcd716f4e350",
              "IPY_MODEL_3a10fcb5c67349c2aaa5daa7f08083df",
              "IPY_MODEL_bd69ba617c484d0cb971eb7d6f4946b0"
            ],
            "layout": "IPY_MODEL_7ab7e3dbfa474f87bb04fd45815dc487"
          }
        },
        "bfaede04cda04fd4b64ffcd716f4e350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34f83e1e081d4b408b55f37efbf2d220",
            "placeholder": "​",
            "style": "IPY_MODEL_f2ba324fc1564df18b5d64b264ec20fc",
            "value": "model.safetensors: 100%"
          }
        },
        "3a10fcb5c67349c2aaa5daa7f08083df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e27bba8a85d437197857d4cc5e8254f",
            "max": 800582904,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_decf1e33f61a4c7db13afb479bc9d30b",
            "value": 800582904
          }
        },
        "bd69ba617c484d0cb971eb7d6f4946b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ec723b7b5a24c5ca75bca1b44d57f11",
            "placeholder": "​",
            "style": "IPY_MODEL_5657dca73d4948eca41e9a871adb9db6",
            "value": " 801M/801M [00:03&lt;00:00, 277MB/s]"
          }
        },
        "7ab7e3dbfa474f87bb04fd45815dc487": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34f83e1e081d4b408b55f37efbf2d220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2ba324fc1564df18b5d64b264ec20fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e27bba8a85d437197857d4cc5e8254f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "decf1e33f61a4c7db13afb479bc9d30b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4ec723b7b5a24c5ca75bca1b44d57f11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5657dca73d4948eca41e9a871adb9db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e371c5590dae403f913d3c56d464d2c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a877ee1afca04754aefcb82d237538a4",
              "IPY_MODEL_cdaca1c3b30d4510859b2f10ed73c01d",
              "IPY_MODEL_fee563515bb74b76b3744bec1769780b"
            ],
            "layout": "IPY_MODEL_621e9657ae4740519cff17b2c236a397"
          }
        },
        "a877ee1afca04754aefcb82d237538a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f96da5e5a9ac43fab6e98e32164c9597",
            "placeholder": "​",
            "style": "IPY_MODEL_0eb5f878d3c94406a4a0e53b893e447a",
            "value": "model.safetensors: 100%"
          }
        },
        "cdaca1c3b30d4510859b2f10ed73c01d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34cd1776d06c45b7a936a32b6a370f01",
            "max": 800582904,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f7cfc111d434341a96ae0a94cbc702e",
            "value": 800582904
          }
        },
        "fee563515bb74b76b3744bec1769780b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b76eaceb1da3481f8d5d594d72015579",
            "placeholder": "​",
            "style": "IPY_MODEL_f363609dfeb5450bbee4253f9b664a8a",
            "value": " 801M/801M [00:03&lt;00:00, 264MB/s]"
          }
        },
        "621e9657ae4740519cff17b2c236a397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f96da5e5a9ac43fab6e98e32164c9597": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0eb5f878d3c94406a4a0e53b893e447a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34cd1776d06c45b7a936a32b6a370f01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f7cfc111d434341a96ae0a94cbc702e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b76eaceb1da3481f8d5d594d72015579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f363609dfeb5450bbee4253f9b664a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3c1204d4074404da883b86224221a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da23dafff5b744d0b82778234f571695",
              "IPY_MODEL_abfb456bfb4a4484aacac61720c8f5bc",
              "IPY_MODEL_b3eb1e4c8241429e912c54cedcf5b625"
            ],
            "layout": "IPY_MODEL_478cb325deb3464aa632220a58b56421"
          }
        },
        "da23dafff5b744d0b82778234f571695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d6e3a91db3446fba374acb588fb88ff",
            "placeholder": "​",
            "style": "IPY_MODEL_4bb04587d4d14222b292b4ff0f1ff824",
            "value": "model.safetensors: 100%"
          }
        },
        "abfb456bfb4a4484aacac61720c8f5bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_990d7ce5f0304d4fbdb8e3649563b461",
            "max": 49335454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2d7fea344ea4c07b58b1905240eab5b",
            "value": 49335454
          }
        },
        "b3eb1e4c8241429e912c54cedcf5b625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c090408547d745aeb7956d047e7602da",
            "placeholder": "​",
            "style": "IPY_MODEL_9b4b64cd88004e0ab396aaadb50457cd",
            "value": " 49.3M/49.3M [00:00&lt;00:00, 159MB/s]"
          }
        },
        "478cb325deb3464aa632220a58b56421": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d6e3a91db3446fba374acb588fb88ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bb04587d4d14222b292b4ff0f1ff824": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "990d7ce5f0304d4fbdb8e3649563b461": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2d7fea344ea4c07b58b1905240eab5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c090408547d745aeb7956d047e7602da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b4b64cd88004e0ab396aaadb50457cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bff2d2cf3ea47339287e5509d320526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d066c906eda485fa89e730d2c964faa",
              "IPY_MODEL_a08e103c04d2409e9a43f992ca05da8f",
              "IPY_MODEL_67aa03d3050a4af8988c6cc38a60ed66"
            ],
            "layout": "IPY_MODEL_bd5f8728ca194c6383efdc730a2dbd2a"
          }
        },
        "1d066c906eda485fa89e730d2c964faa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cecd21ca1d554da69b7810d93519a7df",
            "placeholder": "​",
            "style": "IPY_MODEL_afd371f312f740149fb6c447ffb155c8",
            "value": "model.safetensors: 100%"
          }
        },
        "a08e103c04d2409e9a43f992ca05da8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7501c2e30fb74ffc8c2d0a688737d544",
            "max": 32334434,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_749edb6859ed40fea66c53f567a80c45",
            "value": 32334434
          }
        },
        "67aa03d3050a4af8988c6cc38a60ed66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3d32c24053e46f7b5d360e4c6a2ba3b",
            "placeholder": "​",
            "style": "IPY_MODEL_9d0b36769b15439bbfdb4e325ca105c0",
            "value": " 32.3M/32.3M [00:00&lt;00:00, 152MB/s]"
          }
        },
        "bd5f8728ca194c6383efdc730a2dbd2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cecd21ca1d554da69b7810d93519a7df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afd371f312f740149fb6c447ffb155c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7501c2e30fb74ffc8c2d0a688737d544": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "749edb6859ed40fea66c53f567a80c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3d32c24053e46f7b5d360e4c6a2ba3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d0b36769b15439bbfdb4e325ca105c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d501f384672946d4947dab7598f54d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5744087039d7475fb832e88ce51f1ee1",
              "IPY_MODEL_02b414a831b8441c945b90eae39b770b",
              "IPY_MODEL_06887e4efc834e1b885cd4b5fc866c08"
            ],
            "layout": "IPY_MODEL_81bc53dc5c524a089db6e4517d452dc8"
          }
        },
        "5744087039d7475fb832e88ce51f1ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc574cce2f4e4fe4bf02cf6a493bee42",
            "placeholder": "​",
            "style": "IPY_MODEL_73127dd16b874d27b3ba3f3568297875",
            "value": "model.safetensors: 100%"
          }
        },
        "02b414a831b8441c945b90eae39b770b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b02d4dc537bf4f4b8608921b2f40aca6",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7c44d5d07e0f43e993f98a96dd3c456b",
            "value": 346284714
          }
        },
        "06887e4efc834e1b885cd4b5fc866c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d91528097d3848c1bdb5fa75b080977c",
            "placeholder": "​",
            "style": "IPY_MODEL_31597833e6ad4f4298e1220fa1a70224",
            "value": " 346M/346M [00:01&lt;00:00, 254MB/s]"
          }
        },
        "81bc53dc5c524a089db6e4517d452dc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc574cce2f4e4fe4bf02cf6a493bee42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73127dd16b874d27b3ba3f3568297875": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b02d4dc537bf4f4b8608921b2f40aca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c44d5d07e0f43e993f98a96dd3c456b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d91528097d3848c1bdb5fa75b080977c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31597833e6ad4f4298e1220fa1a70224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "861193351aa64edcb8ebedf1e6a8b954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f3e9707a280482dbb1038f4d8cce71f",
              "IPY_MODEL_fa8a138a8f9942138a2c33166dba694d",
              "IPY_MODEL_cd9d90cf066e449dbfe0667e6af1cc38"
            ],
            "layout": "IPY_MODEL_99af4260364848718a889c42b0b10730"
          }
        },
        "7f3e9707a280482dbb1038f4d8cce71f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35a1d6bb63b744388dd6a203b81852ec",
            "placeholder": "​",
            "style": "IPY_MODEL_83191c0995294e3fbaa6939b2936900d",
            "value": "model.safetensors: 100%"
          }
        },
        "fa8a138a8f9942138a2c33166dba694d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71b547f5bccb4adf83ca2bd4a4cb5bb9",
            "max": 354400320,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8af51e3d2c304c578332bbd3de6963b2",
            "value": 354400320
          }
        },
        "cd9d90cf066e449dbfe0667e6af1cc38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cba1d54aac3944b0a393ea3d82bc3df5",
            "placeholder": "​",
            "style": "IPY_MODEL_6bf04142c91d4966b120059a115b57ab",
            "value": " 354M/354M [00:02&lt;00:00, 156MB/s]"
          }
        },
        "99af4260364848718a889c42b0b10730": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35a1d6bb63b744388dd6a203b81852ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83191c0995294e3fbaa6939b2936900d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71b547f5bccb4adf83ca2bd4a4cb5bb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8af51e3d2c304c578332bbd3de6963b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cba1d54aac3944b0a393ea3d82bc3df5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bf04142c91d4966b120059a115b57ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93996f2ca7444514a86e6c3b7b83c35a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_99da7b6b4da44709896133e8e1e9872f",
              "IPY_MODEL_f45a94fd3757464a84f53b54213d2000",
              "IPY_MODEL_c273e283b94a459aa8409bcd4bbc711a"
            ],
            "layout": "IPY_MODEL_e1eeec3f0c5a4fc3a135b5ebcaaef354"
          }
        },
        "99da7b6b4da44709896133e8e1e9872f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f19e407839fd442ea7c50883f7b5b4d4",
            "placeholder": "​",
            "style": "IPY_MODEL_bce23299cd0744bc90e4ac3a451aae0c",
            "value": "Best trial: 1. Best value: 0.879452: 100%"
          }
        },
        "f45a94fd3757464a84f53b54213d2000": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6c0a81e303947cb96ecf58a4c374c9c",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_730488720ca64b89a088ed6b32fc38fa",
            "value": 5
          }
        },
        "c273e283b94a459aa8409bcd4bbc711a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d017ec99d0094e4ba2caf745f0bc9129",
            "placeholder": "​",
            "style": "IPY_MODEL_117e9d02b0ce4990ad2f867d9f0c96cb",
            "value": " 5/5 [59:03&lt;00:00, 671.90s/it]"
          }
        },
        "e1eeec3f0c5a4fc3a135b5ebcaaef354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f19e407839fd442ea7c50883f7b5b4d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bce23299cd0744bc90e4ac3a451aae0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6c0a81e303947cb96ecf58a4c374c9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "730488720ca64b89a088ed6b32fc38fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d017ec99d0094e4ba2caf745f0bc9129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "117e9d02b0ce4990ad2f867d9f0c96cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb4d77d2395947d7875d0cabebb06425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2693cf498c4d4281ac47483138fc9092",
              "IPY_MODEL_eb218795a74840f5b35593a1b9799c34",
              "IPY_MODEL_960084f594fa4b6fadc52acac09d19d3"
            ],
            "layout": "IPY_MODEL_6e7b9da50f62438089712df56c07eb59"
          }
        },
        "2693cf498c4d4281ac47483138fc9092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f1d45c1620a47b5aeea1c496520e69b",
            "placeholder": "​",
            "style": "IPY_MODEL_9ab50d37881b48fab0ffd98f6a7c3e3e",
            "value": "model.safetensors: 100%"
          }
        },
        "eb218795a74840f5b35593a1b9799c34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b97f515ddf9847959cb4bfa754a0a11d",
            "max": 800582904,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_69df5b7207e241c58cf070025e524921",
            "value": 800582904
          }
        },
        "960084f594fa4b6fadc52acac09d19d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79ceb1a22f064ce893e42691c8e31431",
            "placeholder": "​",
            "style": "IPY_MODEL_4959bd5f3cd149ccafafd20e80aecc68",
            "value": " 801M/801M [00:02&lt;00:00, 272MB/s]"
          }
        },
        "6e7b9da50f62438089712df56c07eb59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f1d45c1620a47b5aeea1c496520e69b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ab50d37881b48fab0ffd98f6a7c3e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b97f515ddf9847959cb4bfa754a0a11d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69df5b7207e241c58cf070025e524921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79ceb1a22f064ce893e42691c8e31431": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4959bd5f3cd149ccafafd20e80aecc68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeongjaeSSHIN/SW.AI_DL_17/blob/main/DR_OnDAT_swin_2_ipynb_%EC%B5%9C%EC%A2%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DS-InRQLhsdP",
        "outputId": "01774d22-99f1-484f-e5d9-88952182bf35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''class0 1805장\n",
        "class1 370장\n",
        "class2 999장\n",
        "class3 193장\n",
        "class4 295장'''"
      ],
      "metadata": {
        "id": "EaVHYiyZmg_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch==2.0.1 torchvision==0.15.2 timm optuna albumentations\n",
        "\n",
        "# 1) Colab 런타임을 재시작한 뒤 실행할 셀\n",
        "!pip install numpy opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations optuna"
      ],
      "metadata": {
        "id": "YdcjUt3fkvWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''swin transformer optuna'''\n",
        "\n",
        "# Colab에서 실행 시 필요한 라이브러리 설치\n",
        "# !pip install opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations optuna\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import optuna\n",
        "\n",
        "# -------------------------------\n",
        "# 0) Reproducibility: Seed 고정\n",
        "# -------------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# -------------------------------\n",
        "# 1) 전처리 함수 정의\n",
        "# -------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384,384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15,15))\n",
        "    closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h,w = img_bgr.shape[:2]\n",
        "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) >= h*w*0.1:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx,cy),r = cv2.minEnclosingCircle(cnt)\n",
        "        cx,cy,r = map(int,(cx,cy,r))\n",
        "        mask = np.zeros_like(gray); cv2.circle(mask,(cx,cy),r,255,-1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        img_bgr = masked[max(cy-r,0):min(cy+r,h), max(cx-r,0):min(cx+r,w)]\n",
        "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
        "    l,a_,b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l = clahe.apply(l)\n",
        "    lab = cv2.merge([l,a_,b])\n",
        "    img_bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "    if output_size:\n",
        "        img_bgr = cv2.resize(img_bgr, output_size, interpolation=cv2.INTER_AREA)\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Dataset 정의\n",
        "# -------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname += '.png'\n",
        "        img = np.array(preprocess_fundus_image(\n",
        "            os.path.join(self.img_dir, fname),\n",
        "            output_size=self.preprocess_size))\n",
        "        label = int(row['label'])\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# -------------------------------\n",
        "# 3) 데이터 로드 & Split\n",
        "# -------------------------------\n",
        "ROOT = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "df_all = pd.read_csv(os.path.join(ROOT, 'train.csv')).rename(columns={'diagnosis':'label'})\n",
        "df_all = df_all[df_all['image'].apply(\n",
        "    lambda f: os.path.isfile(os.path.join(ROOT,'train_images',\n",
        "        f if f.lower().endswith('.png') else f+'.png'))\n",
        ")].reset_index(drop=True)\n",
        "\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "tr_idx, hd_idx = next(sss1.split(df_all, df_all['label']))\n",
        "df_train = df_all.iloc[tr_idx].reset_index(drop=True)\n",
        "df_hold  = df_all.iloc[hd_idx].reset_index(drop=True)\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Sampler & Loader 함수\n",
        "# -------------------------------\n",
        "def make_sampler(df):\n",
        "    counts = df['label'].value_counts().sort_index().values\n",
        "    weights = 1.0 / counts\n",
        "    sample_w = df['label'].apply(lambda x: weights[x]).tolist()\n",
        "    return WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n",
        "\n",
        "def make_loaders(batch_size):\n",
        "    sampler = make_sampler(df_train)\n",
        "    train_ds = APTOSDataset(df_train, os.path.join(ROOT,'train_images'))\n",
        "    val_ds   = APTOSDataset(df_val,   os.path.join(ROOT,'train_images'))\n",
        "    test_ds  = APTOSDataset(df_test,  os.path.join(ROOT,'train_images'))\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size=batch_size, sampler=sampler,\n",
        "                   num_workers=4, pin_memory=True),\n",
        "        DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
        "                   num_workers=4, pin_memory=True),\n",
        "        DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
        "                   num_workers=4, pin_memory=True),\n",
        "    )\n",
        "\n",
        "# -------------------------------\n",
        "# 5) Optuna Objective (빠른 탐색용)\n",
        "# -------------------------------\n",
        "EPOCH_TRIAL = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def objective(trial):\n",
        "    dp_rate    = trial.suggest_float('drop_path_rate', 0.0, 0.3)\n",
        "    lr         = trial.suggest_float('max_lr', 1e-4, 1e-2, log=True)\n",
        "    wd         = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32])\n",
        "\n",
        "    train_loader, valid_loader, _ = make_loaders(batch_size)\n",
        "\n",
        "    model = timm.create_model(\n",
        "        'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "        pretrained=True,\n",
        "        drop_path_rate=dp_rate,\n",
        "        num_classes=5\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=lr,\n",
        "                          steps_per_epoch=len(train_loader),\n",
        "                          epochs=EPOCH_TRIAL, pct_start=0.1, div_factor=25.0)\n",
        "    scaler    = GradScaler()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for _ in range(EPOCH_TRIAL):\n",
        "        model.train()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                loss = criterion(model(imgs), labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in valid_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            pred = model(imgs).argmax(1)\n",
        "            correct += (pred == labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# -------------------------------\n",
        "# 6) Optuna 최적화 실행 & 결과\n",
        "# -------------------------------\n",
        "if __name__ == '__main__':\n",
        "    study = optuna.create_study(\n",
        "        direction='maximize',\n",
        "        pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
        "    )\n",
        "    study.optimize(objective, n_trials=5, show_progress_bar=True)\n",
        "    print(\"Best params:\", study.best_params)\n",
        "    print(\"Best validation accuracy:\", study.best_value)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 7) 최적 파라미터로 전체 50 epoch 재학습 & 테스트 평가\n",
        "    # -------------------------------\n",
        "    best = study.best_params\n",
        "    train_loader, valid_loader, test_loader = make_loaders(best['batch_size'])\n",
        "\n",
        "    model = timm.create_model(\n",
        "        'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "        pretrained=True,\n",
        "        drop_path_rate=best['drop_path_rate'],\n",
        "        num_classes=5\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=best['max_lr'], weight_decay=best['weight_decay'])\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=best['max_lr'],\n",
        "                          steps_per_epoch=len(train_loader), epochs=50,\n",
        "                          pct_start=0.1, div_factor=25.0)\n",
        "    scaler    = GradScaler()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, 51):\n",
        "        model.train()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                loss = criterion(model(imgs), labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "    model.eval()\n",
        "    t_correct = t_total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            pred = model(imgs).argmax(1)\n",
        "            t_correct += (pred == labels).sum().item()\n",
        "            t_total   += labels.size(0)\n",
        "    test_acc = t_correct / t_total\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 865,
          "referenced_widgets": [
            "93996f2ca7444514a86e6c3b7b83c35a",
            "99da7b6b4da44709896133e8e1e9872f",
            "f45a94fd3757464a84f53b54213d2000",
            "c273e283b94a459aa8409bcd4bbc711a",
            "e1eeec3f0c5a4fc3a135b5ebcaaef354",
            "f19e407839fd442ea7c50883f7b5b4d4",
            "bce23299cd0744bc90e4ac3a451aae0c",
            "f6c0a81e303947cb96ecf58a4c374c9c",
            "730488720ca64b89a088ed6b32fc38fa",
            "d017ec99d0094e4ba2caf745f0bc9129",
            "117e9d02b0ce4990ad2f867d9f0c96cb",
            "fb4d77d2395947d7875d0cabebb06425",
            "2693cf498c4d4281ac47483138fc9092",
            "eb218795a74840f5b35593a1b9799c34",
            "960084f594fa4b6fadc52acac09d19d3",
            "6e7b9da50f62438089712df56c07eb59",
            "1f1d45c1620a47b5aeea1c496520e69b",
            "9ab50d37881b48fab0ffd98f6a7c3e3e",
            "b97f515ddf9847959cb4bfa754a0a11d",
            "69df5b7207e241c58cf070025e524921",
            "79ceb1a22f064ce893e42691c8e31431",
            "4959bd5f3cd149ccafafd20e80aecc68"
          ]
        },
        "collapsed": true,
        "id": "dtvMrnZBkkJS",
        "outputId": "0f5b87a6-1a9a-4600-8fac-147e8161e6f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-06-12 00:44:44,680] A new study created in memory with name: no-name-c7d3b26b-7c2e-4753-8f41-fa259f712192\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93996f2ca7444514a86e6c3b7b83c35a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/801M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb4d77d2395947d7875d0cabebb06425"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-1443513139>:172: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler    = GradScaler()\n",
            "<ipython-input-3-1443513139>:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[I 2025-06-12 01:00:30,333] Trial 0 finished with value: 0.863013698630137 and parameters: {'drop_path_rate': 0.15299544194888767, 'max_lr': 0.00019221751697071296, 'weight_decay': 0.0001160015683094429, 'batch_size': 32}. Best is trial 0 with value: 0.863013698630137.\n",
            "[I 2025-06-12 01:11:41,944] Trial 1 finished with value: 0.8794520547945206 and parameters: {'drop_path_rate': 0.07785408634722764, 'max_lr': 0.00021160391896617083, 'weight_decay': 0.009990978364401213, 'batch_size': 16}. Best is trial 1 with value: 0.8794520547945206.\n",
            "[I 2025-06-12 01:22:29,376] Trial 2 finished with value: 0.8191780821917808 and parameters: {'drop_path_rate': 0.14367705464594271, 'max_lr': 0.00044622715815366773, 'weight_decay': 9.636006850607332e-05, 'batch_size': 16}. Best is trial 1 with value: 0.8794520547945206.\n",
            "[I 2025-06-12 01:33:04,492] Trial 3 finished with value: 0.821917808219178 and parameters: {'drop_path_rate': 0.2672695550866221, 'max_lr': 0.00027591348381561965, 'weight_decay': 0.00042722152446041296, 'batch_size': 16}. Best is trial 1 with value: 0.8794520547945206.\n",
            "[I 2025-06-12 01:43:48,495] Trial 4 finished with value: 0.8712328767123287 and parameters: {'drop_path_rate': 0.09389966563430019, 'max_lr': 0.00011916759443498601, 'weight_decay': 0.001814899187148116, 'batch_size': 32}. Best is trial 1 with value: 0.8794520547945206.\n",
            "Best params: {'drop_path_rate': 0.07785408634722764, 'max_lr': 0.00021160391896617083, 'weight_decay': 0.009990978364401213, 'batch_size': 16}\n",
            "Best validation accuracy: 0.8794520547945206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-1443513139>:226: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler    = GradScaler()\n",
            "<ipython-input-3-1443513139>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1443513139>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m             \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''efficientnet_b4, best parameter'''\n",
        "\n",
        "# Colab에서 실행 시 필요한 라이브러리 설치\n",
        "# !pip install opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations optuna\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "import optuna\n",
        "\n",
        "# -------------------------------\n",
        "# 0) Reproducibility: Seed 고정\n",
        "# -------------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# -------------------------------\n",
        "# 1) 전처리 함수 정의\n",
        "# -------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(300,300)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"Cannot read image: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15,15))\n",
        "    closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h,w = img_bgr.shape[:2]\n",
        "    # crop circle if large enough\n",
        "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) >= h*w*0.1:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx,cy), r = cv2.minEnclosingCircle(cnt)\n",
        "        cx,cy,r = map(int,(cx,cy,r))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx,cy), r, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        img_bgr = masked[max(cy-r,0):min(cy+r,h), max(cx-r,0):min(cx+r,w)]\n",
        "    # CLAHE in LAB\n",
        "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
        "    l,a_,b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l = clahe.apply(l)\n",
        "    lab = cv2.merge([l,a_,b])\n",
        "    img_bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "    # resize\n",
        "    if output_size:\n",
        "        img_bgr = cv2.resize(img_bgr, output_size, interpolation=cv2.INTER_AREA)\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Dataset 정의\n",
        "# -------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(300,300)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname += '.png'\n",
        "        img = np.array(preprocess_fundus_image(\n",
        "            os.path.join(self.img_dir, fname),\n",
        "            output_size=self.preprocess_size))\n",
        "        label = int(row['label'])\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# -------------------------------\n",
        "# 3) 데이터 로드 & Split\n",
        "# -------------------------------\n",
        "ROOT = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "df_all = pd.read_csv(os.path.join(ROOT, 'train.csv')).rename(columns={'diagnosis':'label'})\n",
        "df_all = df_all[df_all['image'].apply(\n",
        "    lambda f: os.path.isfile(os.path.join(ROOT,'train_images',\n",
        "        f if f.lower().endswith('.png') else f+'.png'))\n",
        ")].reset_index(drop=True)\n",
        "\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "tr_idx, hd_idx = next(sss1.split(df_all, df_all['label']))\n",
        "df_train = df_all.iloc[tr_idx].reset_index(drop=True)\n",
        "df_hold  = df_all.iloc[hd_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Sampler & Loader 함수\n",
        "# -------------------------------\n",
        "def make_sampler(df):\n",
        "    counts = df['label'].value_counts().sort_index().values\n",
        "    weights = 1.0 / counts\n",
        "    sample_w = df['label'].apply(lambda x: weights[x]).tolist()\n",
        "    return WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n",
        "\n",
        "def make_loaders(batch_size):\n",
        "    sampler    = make_sampler(df_train)\n",
        "    train_ds   = APTOSDataset(df_train, os.path.join(ROOT,'train_images'))\n",
        "    val_ds     = APTOSDataset(df_val,   os.path.join(ROOT,'train_images'))\n",
        "    test_ds    = APTOSDataset(df_test,  os.path.join(ROOT,'train_images'))\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size=batch_size, sampler=sampler,   num_workers=4, pin_memory=True),\n",
        "        DataLoader(val_ds,   batch_size=batch_size, shuffle=False,     num_workers=4, pin_memory=True),\n",
        "        DataLoader(test_ds,  batch_size=batch_size, shuffle=False,     num_workers=4, pin_memory=True),\n",
        "    )\n",
        "\n",
        "# -------------------------------\n",
        "# 5) Optuna Objective (빠른 탐색용)\n",
        "# -------------------------------\n",
        "EPOCH_TRIAL = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def objective(trial):\n",
        "    lr         = trial.suggest_float('max_lr',        1e-4, 1e-2, log=True)\n",
        "    wd         = trial.suggest_float('weight_decay',  1e-5, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16,32])\n",
        "\n",
        "    train_loader, valid_loader, _ = make_loaders(batch_size)\n",
        "\n",
        "    model = timm.create_model(\n",
        "        'efficientnet_b3',\n",
        "        pretrained=True,\n",
        "        num_classes=5\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=lr,\n",
        "                          steps_per_epoch=len(train_loader),\n",
        "                          epochs=EPOCH_TRIAL, pct_start=0.1, div_factor=25.0)\n",
        "    scaler    = GradScaler()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 빠른 학습\n",
        "    for _ in range(EPOCH_TRIAL):\n",
        "        model.train()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                loss = criterion(model(imgs), labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "    # 검증 정확도\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in valid_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            pred = model(imgs).argmax(1)\n",
        "            correct += (pred==labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "    return correct/total\n",
        "\n",
        "# -------------------------------\n",
        "# 6) Optuna 최적화 실행 & 결과\n",
        "# -------------------------------\n",
        "if __name__ == '__main__':\n",
        "    study = optuna.create_study(direction='maximize',\n",
        "                                pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
        "    study.optimize(objective, n_trials=5, show_progress_bar=True)\n",
        "    print(\"Best params:\", study.best_params)\n",
        "    print(\"Best validation accuracy:\", study.best_value)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 7) 최적 파라미터로 전체 50 epoch 재학습 & 테스트 평가\n",
        "    # -------------------------------\n",
        "    best = study.best_params\n",
        "    train_loader, valid_loader, test_loader = make_loaders(best['batch_size'])\n",
        "\n",
        "    model = timm.create_model(\n",
        "        'efficientnet_b3',\n",
        "        pretrained=True,\n",
        "        num_classes=5\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=best['max_lr'],      weight_decay=best['weight_decay'])\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=best['max_lr'],\n",
        "                          steps_per_epoch=len(train_loader), epochs=50,\n",
        "                          pct_start=0.1, div_factor=25.0)\n",
        "    scaler    = GradScaler()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1,51):\n",
        "        model.train()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                loss = criterion(model(imgs), labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "'''    # Test 성능\n",
        "    model.eval()\n",
        "    t_correct = t_total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            pred = model(imgs).argmax(1)\n",
        "            t_correct += (pred==labels).sum().item()\n",
        "            t_total   += labels.size(0)\n",
        "    test_acc = t_correct / t_total\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")'''\n"
      ],
      "metadata": {
        "id": "uYLIQbFui0Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''ConvNeXt-Base pretrained, best parameter'''\n",
        "\n",
        "# Colab에서 실행 시 필요한 라이브러리 설치\n",
        "# !pip install opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations optuna\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "import optuna\n",
        "\n",
        "# -------------------------------\n",
        "# 0) Reproducibility: Seed 고정\n",
        "# -------------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# -------------------------------\n",
        "# 1) 전처리 함수 정의\n",
        "# -------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(224,224)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"Cannot read image: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15,15))\n",
        "    closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h,w = img_bgr.shape[:2]\n",
        "    if contours and cv2.contourArea(max(contours, key=cv2.contourArea)) >= h*w*0.1:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx,cy), r = cv2.minEnclosingCircle(cnt)\n",
        "        cx,cy,r = map(int,(cx,cy,r))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask,(cx,cy),r,255,-1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        img_bgr = masked[max(cy-r,0):min(cy+r,h), max(cx-r,0):min(cx+r,w)]\n",
        "    lab = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2LAB)\n",
        "    l,a_,b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l = clahe.apply(l)\n",
        "    lab = cv2.merge([l,a_,b])\n",
        "    img_bgr = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
        "    if output_size:\n",
        "        img_bgr = cv2.resize(img_bgr, output_size, interpolation=cv2.INTER_AREA)\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Dataset 정의\n",
        "# -------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(224,224)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname += '.png'\n",
        "        img = np.array(preprocess_fundus_image(\n",
        "            os.path.join(self.img_dir, fname),\n",
        "            output_size=self.preprocess_size))\n",
        "        label = int(row['label'])\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# -------------------------------\n",
        "# 3) 데이터 로드 & Split\n",
        "# -------------------------------\n",
        "ROOT = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "df_all = pd.read_csv(os.path.join(ROOT, 'train.csv')).rename(columns={'diagnosis':'label'})\n",
        "df_all = df_all[df_all['image'].apply(\n",
        "    lambda f: os.path.isfile(os.path.join(ROOT,'train_images',\n",
        "        f if f.lower().endswith('.png') else f+'.png'))\n",
        ")].reset_index(drop=True)\n",
        "\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "tr_idx, hd_idx = next(sss1.split(df_all, df_all['label']))\n",
        "df_train = df_all.iloc[tr_idx].reset_index(drop=True)\n",
        "df_hold  = df_all.iloc[hd_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Sampler & Loader 함수\n",
        "# -------------------------------\n",
        "def make_sampler(df):\n",
        "    counts   = df['label'].value_counts().sort_index().values\n",
        "    weights  = 1.0 / counts\n",
        "    sample_w = df['label'].apply(lambda x: weights[x]).tolist()\n",
        "    return WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n",
        "\n",
        "def make_loaders(batch_size):\n",
        "    sampler  = make_sampler(df_train)\n",
        "    tr_ds     = APTOSDataset(df_train, os.path.join(ROOT,'train_images'))\n",
        "    v_ds      = APTOSDataset(df_val,   os.path.join(ROOT,'train_images'))\n",
        "    te_ds     = APTOSDataset(df_test,  os.path.join(ROOT,'train_images'))\n",
        "    return (\n",
        "        DataLoader(tr_ds, batch_size=batch_size, sampler=sampler, num_workers=4, pin_memory=True),\n",
        "        DataLoader(v_ds,  batch_size=batch_size, shuffle=False,    num_workers=4, pin_memory=True),\n",
        "        DataLoader(te_ds, batch_size=batch_size, shuffle=False,    num_workers=4, pin_memory=True),\n",
        "    )\n",
        "\n",
        "# -------------------------------\n",
        "# 5) Optuna Objective (빠른 탐색용)\n",
        "# -------------------------------\n",
        "EPOCH_TRIAL = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def objective(trial):\n",
        "    lr         = trial.suggest_float('max_lr',        1e-4, 1e-2, log=True)\n",
        "    wd         = trial.suggest_float('weight_decay',  1e-5, 1e-2, log=True)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16,32])\n",
        "\n",
        "    train_loader, valid_loader, _ = make_loaders(batch_size)\n",
        "\n",
        "    model = timm.create_model(\n",
        "        'convnext_base',\n",
        "        pretrained=True,\n",
        "        num_classes=5\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=lr,\n",
        "                          steps_per_epoch=len(train_loader),\n",
        "                          epochs=EPOCH_TRIAL, pct_start=0.1, div_factor=25.0)\n",
        "    scaler    = GradScaler()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 빠른 학습 루프\n",
        "    for _ in range(EPOCH_TRIAL):\n",
        "        model.train()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                loss = criterion(model(imgs), labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "    # 검증 정확도 계산\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in valid_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            pred = model(imgs).argmax(1)\n",
        "            correct += (pred==labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "    return correct/total\n",
        "\n",
        "# -------------------------------\n",
        "# 6) Optuna 최적화 실행 & 결과\n",
        "# -------------------------------\n",
        "if __name__ == '__main__':\n",
        "    study = optuna.create_study(\n",
        "        direction='maximize',\n",
        "        pruner=optuna.pruners.SuccessiveHalvingPruner()\n",
        "    )\n",
        "    study.optimize(objective, n_trials=5, show_progress_bar=True)\n",
        "    print(\"Best params:\", study.best_params)\n",
        "    print(\"Best validation accuracy:\", study.best_value)\n",
        "\n",
        "    # -------------------------------\n",
        "    # 7) 최적 파라미터로 전체 50 epoch 재학습 & 테스트 평가\n",
        "    # -------------------------------\n",
        "    best = study.best_params\n",
        "    train_loader, valid_loader, test_loader = make_loaders(best['batch_size'])\n",
        "\n",
        "    model = timm.create_model(\n",
        "        'convnext_base',\n",
        "        pretrained=True,\n",
        "        num_classes=5\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=best['max_lr'],      weight_decay=best['weight_decay'])\n",
        "    scheduler = OneCycleLR(optimizer, max_lr=best['max_lr'],\n",
        "                          steps_per_epoch=len(train_loader), epochs=50,\n",
        "                          pct_start=0.1, div_factor=25.0)\n",
        "    scaler    = GradScaler()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1,51):\n",
        "        model.train()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                loss = criterion(model(imgs), labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "'''    # Test 성능\n",
        "    model.eval()\n",
        "    t_correct = t_total = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in test_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            pred = model(imgs).argmax(1)\n",
        "            t_correct += (pred==labels).sum().item()\n",
        "            t_total   += labels.size(0)\n",
        "    test_acc = t_correct / t_total\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")'''"
      ],
      "metadata": {
        "id": "lxVA2p-_kSBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''1. Swin transformer, best parameter\n",
        "seed 고정 APTOS 데이터만 사용, StratifiedShuffleSplit을 두 단계로 사용해 전체 APTOS를 80/10/10으로 나눴고,\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만 거치게 됩니다.\n",
        "WeightedRandomsampler 추가\n",
        "optuna best parameter 적용\n",
        "# 나머지 학습 루프(optimizer, scheduler, loss)는 기본 shuffle + nn.CrossEntropyLoss() 세팅 그대로\n",
        "# APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "# OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# -------------------------------\n",
        "# 실험 재현성 위한 Seed 고정 함수 추가\n",
        "# -------------------------------\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)  # 코드 제일 앞에서 실행!\n",
        "\n",
        "# -------------------------------\n",
        "# 이하 필요한 라이브러리 임포트\n",
        "# -------------------------------\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (On-DAT 클래스별 증강)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 파일 존재 필터링\n",
        "# ----------------------------------------\n",
        "ROOT    = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path= os.path.join(ROOT, 'train.csv')\n",
        "img_dir = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image']\n",
        "    .apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10) - random_state 고정!\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = 1.0 / class_counts\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler  = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (Sampler 적용, batch_size=16)\n",
        "# ----------------------------------------\n",
        "BATCH_SIZE = 16  # Optuna 결과 반영\n",
        "train_ds     = APTOSDataset(df_train, img_dir)\n",
        "val_ds       = APTOSDataset(df_val,   img_dir)\n",
        "test_ds      = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅 (Optuna 최고 파라미터 적용)\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True,\n",
        "    drop_path_rate=0.07785408634722764,  # Optuna 최적값\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.00021160391896617083,         # Optuna 최적 max_lr\n",
        "    weight_decay=0.009990978364401213  # Optuna 최적 weight_decay\n",
        ")\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.00021160391896617083,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", counts)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc\n",
        "        no_improve   = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos_aug_final.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos_aug_final.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5E6gJxy28D7",
        "outputId": "3091a47f-bbc7-4cd0-e697-b125c4693720"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3350818812>:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 1: 100%|██████████| 183/183 [03:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 0.9566, Acc: 0.6011\n",
            "  Class counts: [554, 583, 616, 584, 581]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.5716, Acc: 0.7699\n",
            "  P: 0.6613, R: 0.7129, F1: 0.6525\n",
            "  CM:\n",
            " [[169   9   1   0   0]\n",
            " [  2  30   5   0   0]\n",
            " [  2   5  52  33   8]\n",
            " [  0   0   2  15   2]\n",
            " [  0   1   5   9  15]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [03:19<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.5430, Acc: 0.7875\n",
            "  Class counts: [595, 604, 578, 578, 563]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 0.8337, Acc: 0.6959\n",
            "  P: 0.6447, R: 0.6903, F1: 0.5992\n",
            "  CM:\n",
            " [[156  23   0   0   0]\n",
            " [  0  37   0   0   0]\n",
            " [  1  47  31  12   9]\n",
            " [  0   1   2  14   2]\n",
            " [  0   6   3   5  16]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 0.4877, Acc: 0.8074\n",
            "  Class counts: [582, 577, 562, 612, 585]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 0.4814, Acc: 0.8329\n",
            "  P: 0.7165, R: 0.7792, F1: 0.7286\n",
            "  CM:\n",
            " [[170   9   0   0   0]\n",
            " [  0  36   1   0   0]\n",
            " [  1   6  67  19   7]\n",
            " [  0   0   4  14   1]\n",
            " [  0   1   8   4  17]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:28<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 0.4402, Acc: 0.8328\n",
            "  Class counts: [568, 554, 607, 588, 601]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 0.4281, Acc: 0.8329\n",
            "  P: 0.7088, R: 0.7398, F1: 0.7154\n",
            "  CM:\n",
            " [[177   2   0   0   0]\n",
            " [  3  31   3   0   0]\n",
            " [  3   6  66  16   9]\n",
            " [  0   0   5  11   3]\n",
            " [  0   0   5   6  19]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:25<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 0.3675, Acc: 0.8619\n",
            "  Class counts: [597, 565, 578, 580, 598]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 0.4984, Acc: 0.8301\n",
            "  P: 0.7015, R: 0.6651, F1: 0.6805\n",
            "  CM:\n",
            " [[174   5   0   0   0]\n",
            " [  4  23  10   0   0]\n",
            " [  3   2  83   7   5]\n",
            " [  0   0   6   7   6]\n",
            " [  1   0  11   2  16]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:24<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 0.2979, Acc: 0.8931\n",
            "  Class counts: [580, 572, 570, 582, 614]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 0.4650, Acc: 0.8384\n",
            "  P: 0.7044, R: 0.7520, F1: 0.7223\n",
            "  CM:\n",
            " [[173   5   1   0   0]\n",
            " [  0  31   5   0   1]\n",
            " [  1   6  71  13   9]\n",
            " [  0   0   4  11   4]\n",
            " [  0   1   5   4  20]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 7:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 7: 100%|██████████| 183/183 [03:27<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Train Loss: 0.2656, Acc: 0.9092\n",
            "  Class counts: [593, 555, 604, 547, 619]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Valid Loss: 0.5117, Acc: 0.8384\n",
            "  P: 0.7216, R: 0.6916, F1: 0.7002\n",
            "  CM:\n",
            " [[173   6   0   0   0]\n",
            " [  5  29   3   0   0]\n",
            " [  2   4  82   9   3]\n",
            " [  0   0   9   8   2]\n",
            " [  0   0  13   3  14]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 8:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 8: 100%|██████████| 183/183 [03:27<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 008 — Train Loss: 0.1968, Acc: 0.9291\n",
            "  Class counts: [578, 599, 561, 597, 583]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 008 — Valid Loss: 0.5162, Acc: 0.8356\n",
            "  P: 0.7012, R: 0.7372, F1: 0.7164\n",
            "  CM:\n",
            " [[171   8   0   0   0]\n",
            " [  2  29   5   0   1]\n",
            " [  0   3  74  11  12]\n",
            " [  0   0   7   9   3]\n",
            " [  0   0   6   2  22]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 9:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 9: 100%|██████████| 183/183 [03:29<00:00,  1.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 009 — Train Loss: 0.2062, Acc: 0.9318\n",
            "  Class counts: [579, 547, 634, 590, 568]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 009 — Valid Loss: 0.7142, Acc: 0.8219\n",
            "  P: 0.7268, R: 0.6332, F1: 0.6561\n",
            "  CM:\n",
            " [[173   4   2   0   0]\n",
            " [  0  14  23   0   0]\n",
            " [  0   0  90   8   2]\n",
            " [  0   0   9   8   2]\n",
            " [  0   0  11   4  15]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 10:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 10: 100%|██████████| 183/183 [03:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 010 — Train Loss: 0.1481, Acc: 0.9534\n",
            "  Class counts: [604, 529, 614, 601, 570]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 010 — Valid Loss: 0.5716, Acc: 0.8767\n",
            "  P: 0.7951, R: 0.7578, F1: 0.7724\n",
            "  CM:\n",
            " [[174   5   0   0   0]\n",
            " [  3  30   4   0   0]\n",
            " [  2   3  88   4   3]\n",
            " [  0   0   8  10   1]\n",
            " [  0   0   9   3  18]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 11:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 11: 100%|██████████| 183/183 [03:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 011 — Train Loss: 0.1426, Acc: 0.9465\n",
            "  Class counts: [609, 551, 582, 630, 546]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 011 — Valid Loss: 0.7272, Acc: 0.8219\n",
            "  P: 0.7210, R: 0.6352, F1: 0.6609\n",
            "  CM:\n",
            " [[177   2   0   0   0]\n",
            " [ 10  17   8   0   2]\n",
            " [  5   1  84   8   2]\n",
            " [  0   0   9   8   2]\n",
            " [  1   0  10   5  14]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 12:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 12: 100%|██████████| 183/183 [03:22<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 012 — Train Loss: 0.1314, Acc: 0.9520\n",
            "  Class counts: [564, 604, 600, 579, 571]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 012 — Valid Loss: 0.5719, Acc: 0.8274\n",
            "  P: 0.7339, R: 0.6988, F1: 0.7142\n",
            "  CM:\n",
            " [[176   3   0   0   0]\n",
            " [ 11  21   5   0   0]\n",
            " [  7   6  75   7   5]\n",
            " [  0   0   8  10   1]\n",
            " [  0   1   7   2  20]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 13:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 13: 100%|██████████| 183/183 [03:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 013 — Train Loss: 0.1611, Acc: 0.9387\n",
            "  Class counts: [566, 581, 604, 586, 581]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 013 — Valid Loss: 0.5344, Acc: 0.8438\n",
            "  P: 0.7452, R: 0.7119, F1: 0.7243\n",
            "  CM:\n",
            " [[169   9   1   0   0]\n",
            " [  4  25   8   0   0]\n",
            " [  0   1  88   5   6]\n",
            " [  0   0   9  10   0]\n",
            " [  0   0  11   3  16]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 14:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 14: 100%|██████████| 183/183 [03:20<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 014 — Train Loss: 0.1175, Acc: 0.9599\n",
            "  Class counts: [588, 580, 572, 590, 588]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 014 — Valid Loss: 0.7278, Acc: 0.8082\n",
            "  P: 0.7126, R: 0.6627, F1: 0.6641\n",
            "  CM:\n",
            " [[173   5   1   0   0]\n",
            " [  2  16  15   0   4]\n",
            " [  0   0  76   3  21]\n",
            " [  0   0   5   8   6]\n",
            " [  0   0   7   1  22]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 15:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3350818812>:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 15: 100%|██████████| 183/183 [03:24<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 015 — Train Loss: 0.1022, Acc: 0.9640\n",
            "  Class counts: [591, 605, 597, 576, 549]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 015 — Valid Loss: 0.6812, Acc: 0.8356\n",
            "  P: 0.7028, R: 0.6828, F1: 0.6737\n",
            "  CM:\n",
            " [[173   6   0   0   0]\n",
            " [  1  35   1   0   0]\n",
            " [  0  11  80   6   3]\n",
            " [  0   0  11   7   1]\n",
            " [  1   1  13   5  10]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [00:25<00:00,  1.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance ===\n",
            "Test Loss : 0.4473\n",
            "Test Acc  : 0.8849\n",
            "Precision : 0.7629\n",
            "Recall    : 0.7656\n",
            "F1-score  : 0.7641\n",
            "Confusion Matrix:\n",
            "[[179   1   0   0   0]\n",
            " [  1  30   5   0   1]\n",
            " [  0   7  84   6   2]\n",
            " [  0   0   5   9   6]\n",
            " [  0   0   4   4  21]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' StratifiedShuffleSplit을 두 단계로 사용해 전체 APTOS를 80/10/10\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만\n",
        "WeightedRandomsampler 추가\n",
        "Optuna'''\n",
        "\n",
        "# Colab에서 실행 시 필요한 라이브러리 설치\n",
        "# !pip install opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations optuna\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import optuna\n",
        "\n",
        "# -------------------------------\n",
        "# 0) Reproducibility: Seed 고정\n",
        "# -------------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# -------------------------------\n",
        "# 1) 전처리 함수 정의\n",
        "# -------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384,384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15,15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h,w = img_bgr.shape[:2]\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx,cy),radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx,cy,radius = map(int,(cx,cy,radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask,(cx,cy),radius,255,-1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1,y1 = max(cx-radius,0), max(cy-radius,0)\n",
        "        x2,y2 = min(cx+radius,w), min(cy+radius,h)\n",
        "        cropped = masked[y1:y2,x1:x2]\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l,a_,b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe,a_,b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "    if output_size:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Dataset 정의\n",
        "# -------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15,p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "    def __len__(self): return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname += '.png'\n",
        "        pil = preprocess_fundus_image(os.path.join(self.img_dir,fname), output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "        label = int(row['label'])\n",
        "        if label>=2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# -------------------------------\n",
        "# 3) 데이터 로드 & Split\n",
        "# -------------------------------\n",
        "ROOT = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "df_all = pd.read_csv(os.path.join(ROOT,'train.csv')).rename(columns={'diagnosis':'label'})\n",
        "df_all = df_all[df_all['image'].apply(\n",
        "    lambda f: os.path.isfile(os.path.join(ROOT,'train_images', f if f.lower().endswith('.png') else f+'.png'))\n",
        ")].reset_index(drop=True)\n",
        "\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "tr_idx,hold_idx = next(sss1.split(df_all, df_all['label']))\n",
        "df_train = df_all.iloc[tr_idx].reset_index(drop=True)\n",
        "df_hold  = df_all.iloc[hold_idx].reset_index(drop=True)\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 4) Sampler & Loader 함수\n",
        "# -------------------------------\n",
        "def make_sampler(df):\n",
        "    counts = df['label'].value_counts().sort_index().values\n",
        "    weights = 1.0/counts\n",
        "    sample_w = df['label'].apply(lambda x:weights[x]).tolist()\n",
        "    return WeightedRandomSampler(sample_w, num_samples=len(sample_w), replacement=True)\n",
        "\n",
        "def make_loaders(bs):\n",
        "    samp = make_sampler(df_train)\n",
        "    tr_ds = APTOSDataset(df_train, os.path.join(ROOT,'train_images'))\n",
        "    v_ds  = APTOSDataset(df_val,   os.path.join(ROOT,'train_images'))\n",
        "    te_ds = APTOSDataset(df_test,  os.path.join(ROOT,'train_images'))\n",
        "    return (\n",
        "      DataLoader(tr_ds, batch_size=bs, sampler=samp, num_workers=4, pin_memory=True),\n",
        "      DataLoader(v_ds,  batch_size=bs, shuffle=False, num_workers=4, pin_memory=True),\n",
        "      DataLoader(te_ds, batch_size=bs, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    )\n",
        "\n",
        "# -------------------------------\n",
        "# 5) Optuna Objective\n",
        "# -------------------------------\n",
        "EPOCH_TRIAL = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def objective(trial):\n",
        "    window      = trial.suggest_categorical('window_size',[7,9,11])\n",
        "    dp_rate     = trial.suggest_float('drop_path_rate',0.0,0.3)\n",
        "    mlp_r       = trial.suggest_float('mlp_ratio',2.0,4.0)\n",
        "    lr          = trial.suggest_loguniform('max_lr',1e-4,1e-2)\n",
        "    wd          = trial.suggest_loguniform('weight_decay',1e-5,1e-2)\n",
        "    batch_size  = trial.suggest_categorical('batch_size',[16,32])\n",
        "\n",
        "    train_loader, valid_loader, _ = make_loaders(batch_size)\n",
        "    model = timm.create_model(\n",
        "        f'swin_large_patch4_window{window}_384.ms_in22k_ft_in1k',\n",
        "        pretrained=True,\n",
        "        drop_path_rate=dp_rate,\n",
        "        mlp_ratio=mlp_r,\n",
        "        num_classes=5\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = OneCycleLR(\n",
        "      optimizer, max_lr=lr,\n",
        "      steps_per_epoch=len(train_loader),\n",
        "      epochs=EPOCH_TRIAL, pct_start=0.1, div_factor=25.0\n",
        "    )\n",
        "    scaler = GradScaler()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 짧은 루프\n",
        "    for epoch in range(EPOCH_TRIAL):\n",
        "        model.train()\n",
        "        for imgs,labels in train_loader:\n",
        "            imgs,labels=imgs.to(device),labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            with autocast():\n",
        "                out  = model(imgs)\n",
        "                loss = criterion(out, labels)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer); scaler.update()\n",
        "            scheduler.step()\n",
        "\n",
        "    # Validation 정확도\n",
        "    model.eval()\n",
        "    correct=total=0\n",
        "    with torch.no_grad():\n",
        "        for imgs,labels in valid_loader:\n",
        "            imgs,labels=imgs.to(device),labels.to(device)\n",
        "            pred = model(imgs).argmax(1)\n",
        "            correct += (pred==labels).sum().item()\n",
        "            total   += labels.size(0)\n",
        "    return correct/total\n",
        "\n",
        "# -------------------------------\n",
        "# 6) 최적화 실행 & 결과 출력\n",
        "# -------------------------------\n",
        "if __name__=='__main__':\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=20)\n",
        "    print(\"Best params:\", study.best_params)\n",
        "    print(\"Best validation accuracy:\", study.best_value)\n",
        "\n",
        "# 7) 최적 파라미터로 전체 50 epoch 재학습 & Test 평가\n",
        "best = study.best_params\n",
        "train_loader, valid_loader, test_loader = make_loaders(best['batch_size'])\n",
        "\n",
        "# 7-1) 모델 초기화 (Optuna로 찾은 params 반영)\n",
        "model = timm.create_model(\n",
        "    f\"swin_large_patch4_window{best['window_size']}_384.ms_in22k_ft_in1k\",\n",
        "    pretrained=True,\n",
        "    drop_path_rate=best['drop_path_rate'],\n",
        "    mlp_ratio=best['mlp_ratio'],\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=best['max_lr'], weight_decay=best['weight_decay'])\n",
        "scheduler = OneCycleLR(optimizer, max_lr=best['max_lr'],\n",
        "                      steps_per_epoch=len(train_loader), epochs=50, pct_start=0.1, div_factor=25.0)\n",
        "scaler    = GradScaler()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 7-2) 50 에폭 학습 루프 (기존 코드 재사용)\n",
        "for epoch in range(1, 51):\n",
        "    model.train()\n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            loss = criterion(model(imgs), labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "# 7-3) Test 성능 출력\n",
        "model.eval()\n",
        "t_correct = t_total = 0\n",
        "all_p = all_l = []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out = model(imgs)\n",
        "        pred = out.argmax(1)\n",
        "        t_correct += (pred==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "test_acc = t_correct / t_total\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "ZVGpu1v6fCMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab에서 실행 시, 필요 라이브러리 설치 (Albumentations 예시)\n",
        "#!pip install albumentations opencv-python\n",
        "\n",
        "# 만약 torchvision 0.8 이하로 설치되어 있으면, 최신 버전으로 업그레이드하세요.\n",
        "#!pip install --upgrade torchvision\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Kb1kmb0QiQyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''1차시, 증강 없음 !!!! APTOS 데이터만 사용, StratifiedShuffleSplit을 두 단계로 사용해 전체 APTOS를 80/10/10으로 나눴고,\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# Colab에서 실행 시 필요한 라이브러리 설치\n",
        "# !pip install opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (전처리만, 증강 없음)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "        self.transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil_img = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img_np = np.array(pil_img)\n",
        "        img_t = self.transform(image=img_np)['image']\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 “존재하지 않는 파일” 필터링\n",
        "# ----------------------------------------\n",
        "ROOT    = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path= os.path.join(ROOT, 'train.csv')\n",
        "img_dir = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "\n",
        "# 파일 존재 여부 체크 함수\n",
        "def file_exists(fname):\n",
        "    if not fname.lower().endswith('.png'):\n",
        "        fname = f\"{fname}.png\"\n",
        "    return os.path.isfile(os.path.join(img_dir, fname))\n",
        "\n",
        "# 실제 파일이 있는 행만 남깁니다.\n",
        "full_df = full_df[ full_df['image'].apply(file_exists) ].reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10)\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, holdout_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train   = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_holdout = full_df.iloc[holdout_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_holdout, df_holdout['label']))\n",
        "df_val  = df_holdout.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_holdout.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) Dataset & DataLoader 준비\n",
        "# ----------------------------------------\n",
        "train_ds = APTOSDataset(df_train, img_dir)\n",
        "val_ds   = APTOSDataset(df_val,   img_dir)\n",
        "test_ds  = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    # — Train —\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    class_counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            class_counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    for cls in range(num_classes):\n",
        "        print(f\"  Class {cls}: {class_counts[cls]} samples\")\n",
        "\n",
        "    # — Validate —\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu())\n",
        "            all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  Confusion Matrix:\")\n",
        "    print(v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc\n",
        "        no_improve   = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu())\n",
        "        all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b5f8e8275525418fbc2e33455985afb6",
            "26e73300a97e403692be605aa596e798",
            "5a16994df309473eae456867653dc986",
            "d312644e1e4648e29b7216472752a4ef",
            "3cd7f6f5d38347fea196accc99ebfea5",
            "8cab2ca4536344078de852b1f391e2a4",
            "92b497d742a44c74acb26f2e0513c0ef",
            "06a762004c36439cb69d383d59192232",
            "dd2cf5d576994ec9b55e0149c0ee27ea",
            "049ec5f756e74d2389fb270ea5ee8fdd",
            "13a65424362149ec9adbe65c703d03fd"
          ]
        },
        "id": "KZFK95M_BFPG",
        "outputId": "f5605103-4d8f-4ab0-ee80-13ffe3d01bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/801M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5f8e8275525418fbc2e33455985afb6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-f2093a21cf21>:158: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-2-f2093a21cf21>:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 1: 100%|██████████| 183/183 [08:00<00:00,  2.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 0.6145, Acc: 0.7721\n",
            "  Class 0: 1437 samples\n",
            "  Class 1: 295 samples\n",
            "  Class 2: 796 samples\n",
            "  Class 3: 154 samples\n",
            "  Class 4: 236 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [01:05<00:00,  2.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.4989, Acc: 0.8164\n",
            "  P: 0.8164, R: 0.5662, F1: 0.5760\n",
            "  Confusion Matrix:\n",
            "[[172   7   0   0   0]\n",
            " [  0  21  16   0   0]\n",
            " [  0   4  95   0   1]\n",
            " [  0   0  17   1   1]\n",
            " [  0   2  19   0   9]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-2-f2093a21cf21>:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [03:02<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.5608, Acc: 0.7913\n",
            "  Class 0: 1437 samples\n",
            "  Class 1: 295 samples\n",
            "  Class 2: 796 samples\n",
            "  Class 3: 154 samples\n",
            "  Class 4: 236 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 0.6270, Acc: 0.7671\n",
            "  P: 0.5390, R: 0.5044, F1: 0.4990\n",
            "  Confusion Matrix:\n",
            "[[175   3   1   0   0]\n",
            " [  9   9  19   0   0]\n",
            " [  6   2  88   4   0]\n",
            " [  0   0  11   8   0]\n",
            " [  0   1  27   2   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-2-f2093a21cf21>:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:03<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 0.6524, Acc: 0.7546\n",
            "  Class 0: 1437 samples\n",
            "  Class 1: 295 samples\n",
            "  Class 2: 796 samples\n",
            "  Class 3: 154 samples\n",
            "  Class 4: 236 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 1.1828, Acc: 0.6877\n",
            "  P: 0.3311, R: 0.3678, F1: 0.3446\n",
            "  Confusion Matrix:\n",
            "[[179   0   0   0   0]\n",
            " [ 26   7   4   0   0]\n",
            " [ 23  12  65   0   0]\n",
            " [  0   1  18   0   0]\n",
            " [  5   5  20   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-2-f2093a21cf21>:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:02<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 1.1130, Acc: 0.5822\n",
            "  Class 0: 1437 samples\n",
            "  Class 1: 295 samples\n",
            "  Class 2: 796 samples\n",
            "  Class 3: 154 samples\n",
            "  Class 4: 236 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 1.3414, Acc: 0.2740\n",
            "  P: 0.0548, R: 0.2000, F1: 0.0860\n",
            "  Confusion Matrix:\n",
            "[[  0   0 179   0   0]\n",
            " [  0   0  37   0   0]\n",
            " [  0   0 100   0   0]\n",
            " [  0   0  19   0   0]\n",
            " [  0   0  30   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-2-f2093a21cf21>:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [02:55<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 1.3139, Acc: 0.4856\n",
            "  Class 0: 1437 samples\n",
            "  Class 1: 295 samples\n",
            "  Class 2: 796 samples\n",
            "  Class 3: 154 samples\n",
            "  Class 4: 236 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 1.3110, Acc: 0.4904\n",
            "  P: 0.0981, R: 0.2000, F1: 0.1316\n",
            "  Confusion Matrix:\n",
            "[[179   0   0   0   0]\n",
            " [ 37   0   0   0   0]\n",
            " [100   0   0   0   0]\n",
            " [ 19   0   0   0   0]\n",
            " [ 30   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-2-f2093a21cf21>:188: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:01<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 1.3075, Acc: 0.4880\n",
            "  Class 0: 1437 samples\n",
            "  Class 1: 295 samples\n",
            "  Class 2: 796 samples\n",
            "  Class 3: 154 samples\n",
            "  Class 4: 236 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 1.3104, Acc: 0.4904\n",
            "  P: 0.0981, R: 0.2000, F1: 0.1316\n",
            "  Confusion Matrix:\n",
            "[[179   0   0   0   0]\n",
            " [ 37   0   0   0   0]\n",
            " [100   0   0   0   0]\n",
            " [ 19   0   0   0   0]\n",
            " [ 30   0   0   0   0]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [01:17<00:00,  3.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance ===\n",
            "Test Loss : 0.4578\n",
            "Test Acc  : 0.8466\n",
            "Precision : 0.7875\n",
            "Recall    : 0.6089\n",
            "F1-score  : 0.6396\n",
            "Confusion Matrix:\n",
            "[[180   0   0   0   0]\n",
            " [  1  18  17   0   1]\n",
            " [  0   4  95   0   0]\n",
            " [  0   2  13   3   2]\n",
            " [  0   2  13   1  13]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''(추가 !!!!!!) 1차시, 증강 없음, APTOS 데이터만 사용\n",
        "train_loader에 WeightedRandomSampler를 적용해서 학습 배치마다 클래스 비율을 균등하게 뽑음\n",
        "손실 함수는 nn.CrossEntropyLoss(weight=weights_tensor)를 사용해 클래스별 가중치를 반영한 Weighted CrossEntropyLoss가 적용\n",
        "StratifiedShuffleSplit을 두 단계로 사용해 전체 APTOS를 80/10/10으로 나눴고,\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR (pct_start=0.3, max_lr=5e-4) + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# Colab에서 실행 시 필요한 라이브러리 설치\n",
        "# !pip install opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (전처리만, 증강 없음)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "        self.transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil_img = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img_np = np.array(pil_img)\n",
        "        img_t = self.transform(image=img_np)['image']\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 필터링\n",
        "# ----------------------------------------\n",
        "ROOT    = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path= os.path.join(ROOT, 'train.csv')\n",
        "img_dir = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[ full_df['image'].apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png'))) ]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10)\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, holdout_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train   = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_holdout = full_df.iloc[holdout_idx].reset_index(drop=True)\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_holdout, df_holdout['label']))\n",
        "df_val  = df_holdout.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_holdout.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = [1.0 / c for c in class_counts]\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler  = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) DataLoader 준비\n",
        "# ----------------------------------------\n",
        "train_ds     = APTOSDataset(df_train, img_dir)\n",
        "val_ds       = APTOSDataset(df_val,   img_dir)\n",
        "test_ds      = APTOSDataset(df_test,  img_dir)\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 손실함수 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True, num_classes=5\n",
        ").to(device)\n",
        "\n",
        "# 1) Weighted CrossEntropyLoss\n",
        "weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "# Scheduler 재조정: pct_start=0.3, max_lr=5e-4\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=5e-4,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.3,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    # — Train —\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    epoch_class_counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            epoch_class_counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Epoch Sampling Counts by Class:\", epoch_class_counts)\n",
        "\n",
        "    # — Validate —\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu())\n",
        "            all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  Confusion Matrix:\")\n",
        "    print(v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc\n",
        "        no_improve   = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k', pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu())\n",
        "        all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR0wTnBYVRwr",
        "outputId": "9d813c1a-c7b8-4e0e-c96e-ebee68c63f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2484286396>:162: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2484286396>:193: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "Train Epoch 1: 100%|██████████| 183/183 [03:45<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 0.8389, Acc: 0.5569\n",
            "  Epoch Sampling Counts by Class: [590, 572, 554, 622, 580]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.8920, Acc: 0.6658\n",
            "  P: 0.5937, R: 0.6642, F1: 0.5331\n",
            "  Confusion Matrix:\n",
            "[[163  15   0   0   1]\n",
            " [  1  28   2   1   5]\n",
            " [  1  13  15  21  50]\n",
            " [  0   0   0  14   5]\n",
            " [  0   3   0   4  23]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2484286396>:193: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [03:31<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.4090, Acc: 0.7899\n",
            "  Epoch Sampling Counts by Class: [616, 543, 619, 558, 582]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 0.8794, Acc: 0.7890\n",
            "  P: 0.6573, R: 0.7141, F1: 0.6719\n",
            "  Confusion Matrix:\n",
            "[[166  13   0   0   0]\n",
            " [  0  30   7   0   0]\n",
            " [  1  14  62  15   8]\n",
            " [  0   0   4  11   4]\n",
            " [  0   4   5   2  19]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2484286396>:193: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:35<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 0.3047, Acc: 0.8492\n",
            "  Epoch Sampling Counts by Class: [561, 586, 569, 589, 613]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 1.0611, Acc: 0.7644\n",
            "  P: 0.6809, R: 0.6806, F1: 0.6426\n",
            "  Confusion Matrix:\n",
            "[[172   7   0   0   0]\n",
            " [  1  35   1   0   0]\n",
            " [  1  42  47   9   1]\n",
            " [  0   0   6  10   3]\n",
            " [  0   6   6   3  15]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2484286396>:193: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:25<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 0.2990, Acc: 0.8413\n",
            "  Epoch Sampling Counts by Class: [562, 609, 604, 580, 563]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 0.8251, Acc: 0.7397\n",
            "  P: 0.6065, R: 0.6706, F1: 0.5974\n",
            "  Confusion Matrix:\n",
            "[[172   7   0   0   0]\n",
            " [  1  33   2   0   1]\n",
            " [  2  27  36  14  21]\n",
            " [  0   0   3   9   7]\n",
            " [  0   5   2   3  20]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2484286396>:193: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:27<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 0.3089, Acc: 0.8516\n",
            "  Epoch Sampling Counts by Class: [602, 544, 593, 550, 629]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 1.2213, Acc: 0.7534\n",
            "  P: 0.6865, R: 0.6881, F1: 0.6204\n",
            "  Confusion Matrix:\n",
            "[[172   7   0   0   0]\n",
            " [  2  35   0   0   0]\n",
            " [  2  37  43  17   1]\n",
            " [  0   0   4  14   1]\n",
            " [  0   7   4   8  11]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2484286396>:193: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:25<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 0.2344, Acc: 0.8831\n",
            "  Epoch Sampling Counts by Class: [593, 566, 566, 591, 602]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 1.2340, Acc: 0.7781\n",
            "  P: 0.6021, R: 0.6355, F1: 0.6138\n",
            "  Confusion Matrix:\n",
            "[[175   3   1   0   0]\n",
            " [  1  28   7   0   1]\n",
            " [  2  14  58  11  15]\n",
            " [  0   0  10   5   4]\n",
            " [  0   3   6   3  18]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 7:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2484286396>:193: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 7: 100%|██████████| 183/183 [03:22<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Train Loss: 0.2112, Acc: 0.8982\n",
            "  Epoch Sampling Counts by Class: [614, 568, 566, 581, 589]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Valid Loss: 1.0701, Acc: 0.7699\n",
            "  P: 0.6449, R: 0.7203, F1: 0.6483\n",
            "  Confusion Matrix:\n",
            "[[173   3   1   0   2]\n",
            " [  2  22   7   0   6]\n",
            " [  0   9  47  20  24]\n",
            " [  0   0   3  14   2]\n",
            " [  0   0   1   4  25]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [01:25<00:00,  3.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance ===\n",
            "Test Loss : 0.9141\n",
            "Test Acc  : 0.8219\n",
            "Precision : 0.6685\n",
            "Recall    : 0.7132\n",
            "F1-score  : 0.6838\n",
            "Confusion Matrix:\n",
            "[[178   2   0   0   0]\n",
            " [  0  28   8   0   1]\n",
            " [  0  15  64  11   9]\n",
            " [  0   0   3   9   8]\n",
            " [  0   1   3   4  21]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''(추가!!!!!!!) 1차시, 증강 없음 !!!! APTOS 데이터만 사용\n",
        "Focal loss 적용 - 파라미터 좀 바꿈\n",
        "max_lr=5e-4\n",
        "pct_start=0.2 로 전체 스케줄의 20% 구간(첫 10 epochs 중 약 2 epoch)을 워밍업으로 사용\n",
        "StratifiedShuffleSplit을 두 단계로 사용해 전체 APTOS를 80/10/10으로 나눴고,\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# Colab에서 실행 시 필요한 라이브러리 설치\n",
        "# !pip install opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 0) Focal Loss 구현 (gamma, alpha 지원)\n",
        "# ----------------------------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=1.0, alpha=None, reduction='mean'):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha  # tensor of shape [C] or None\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        log_p = nn.functional.log_softmax(input, dim=1)  # [B, C]\n",
        "        p = torch.exp(log_p)                             # [B, C]\n",
        "        target_one_hot = nn.functional.one_hot(target, num_classes=input.size(1)).float().to(input.device)  # [B, C]\n",
        "\n",
        "        # focal term\n",
        "        focal_term = (1 - p) ** self.gamma\n",
        "        loss = -target_one_hot * focal_term * log_p      # [B, C]\n",
        "\n",
        "        if self.alpha is not None:\n",
        "            # α 조정: 각 클래스별 가중치 곱하기\n",
        "            loss = loss * self.alpha.unsqueeze(0)\n",
        "\n",
        "        loss = loss.sum(dim=1)  # [B]\n",
        "        if self.reduction == 'mean':\n",
        "            return loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h * w * 0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (전처리만, 증강 없음)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "        self.transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil_img = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img_np = np.array(pil_img)\n",
        "        img_t = self.transform(image=img_np)['image']\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 필터링\n",
        "# ----------------------------------------\n",
        "ROOT    = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path= os.path.join(ROOT, 'train.csv')\n",
        "img_dir = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image'].apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10)\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) DataLoader 준비\n",
        "# ----------------------------------------\n",
        "train_ds = APTOSDataset(df_train, img_dir)\n",
        "val_ds   = APTOSDataset(df_val,   img_dir)\n",
        "test_ds  = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,  num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) 모델 · 손실함수 · 옵티마이저 · 스케줄러 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True, num_classes=5\n",
        ").to(device)\n",
        "\n",
        "# class frequency 기반 alpha 계산 (역수 비율)\n",
        "class_counts = df_train['label'].value_counts().sort_index().values\n",
        "inv_freq = 1.0 / class_counts\n",
        "alpha = torch.tensor(inv_freq / inv_freq.sum(), device=device)  # 합 = 1\n",
        "\n",
        "criterion = FocalLoss(gamma=1.0, alpha=alpha, reduction='mean')\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=5e-4,           # max_lr 낮춤\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.2,         # 워밍업 구간 늘림\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    class_counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f\"Train {epoch}\"):\n",
        "        for l in labels.cpu().tolist():\n",
        "            class_counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:02d} Train Loss:{train_loss:.4f} Acc:{train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", class_counts)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc=\"Valid\"):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu())\n",
        "            all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:02d} Valid Loss:{valid_loss:.4f} Acc:{valid_acc:.4f}\")\n",
        "    print(f\"  P:{v_prec:.4f} R:{v_rec:.4f} F1:{v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc\n",
        "        no_improve   = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos_focal_alpha.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos_focal_alpha.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc=\"Test\"):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu())\n",
        "        all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZOsU9pN9V3f",
        "outputId": "c4431038-14fd-4abb-c053-f33fd926e7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2252220934>:195: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2252220934>:225: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train 1: 100%|██████████| 183/183 [02:48<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 01 Train Loss:0.0794 Acc:0.6611\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 01 Valid Loss:0.0577 Acc:0.7370\n",
            "  P:0.7026 R:0.6776 F1:0.6066\n",
            "  CM:\n",
            " [[167  12   0   0   0]\n",
            " [  0  28   9   0   0]\n",
            " [  1  12  47  40   0]\n",
            " [  0   0   2  17   0]\n",
            " [  0   3   3  14  10]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2252220934>:225: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train 2: 100%|██████████| 183/183 [02:49<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 02 Train Loss:0.0563 Acc:0.7738\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 02 Valid Loss:0.0570 Acc:0.8110\n",
            "  P:0.7070 R:0.6890 F1:0.6650\n",
            "  CM:\n",
            " [[178   1   0   0   0]\n",
            " [  4  22  11   0   0]\n",
            " [  1   8  70  19   2]\n",
            " [  0   0   3  15   1]\n",
            " [  0   2   8   9  11]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2252220934>:225: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train 3: 100%|██████████| 183/183 [02:49<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 03 Train Loss:0.0502 Acc:0.7937\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 03 Valid Loss:0.0692 Acc:0.7973\n",
            "  P:0.6975 R:0.6622 F1:0.6191\n",
            "  CM:\n",
            " [[175   3   1   0   0]\n",
            " [  4  27   6   0   0]\n",
            " [  1  14  70  14   1]\n",
            " [  0   0   5  14   0]\n",
            " [  0   4   9  12   5]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2252220934>:225: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train 4: 100%|██████████| 183/183 [02:49<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 04 Train Loss:0.0463 Acc:0.8050\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 04 Valid Loss:0.0651 Acc:0.6767\n",
            "  P:0.6090 R:0.6525 F1:0.5968\n",
            "  CM:\n",
            " [[129  44   6   0   0]\n",
            " [  0  28   9   0   0]\n",
            " [  0  15  62  18   5]\n",
            " [  0   0   4  12   3]\n",
            " [  0   5   5   4  16]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2252220934>:225: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train 5: 100%|██████████| 183/183 [02:51<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 05 Train Loss:0.0506 Acc:0.7995\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 05 Valid Loss:0.0591 Acc:0.7178\n",
            "  P:0.6087 R:0.6687 F1:0.5782\n",
            "  CM:\n",
            " [[173   6   0   0   0]\n",
            " [  6  31   0   0   0]\n",
            " [  5  29  26  20  20]\n",
            " [  0   0   2  11   6]\n",
            " [  0   5   1   3  21]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2252220934>:225: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train 6: 100%|██████████| 183/183 [02:48<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 06 Train Loss:0.0525 Acc:0.7875\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 06 Valid Loss:0.0769 Acc:0.7425\n",
            "  P:0.5989 R:0.6166 F1:0.5709\n",
            "  CM:\n",
            " [[176   3   0   0   0]\n",
            " [  4  26   7   0   0]\n",
            " [  5  21  42   5  27]\n",
            " [  0   0   3   4  12]\n",
            " [  0   6   0   1  23]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain 7:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2252220934>:225: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train 7: 100%|██████████| 183/183 [02:49<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 07 Train Loss:0.0556 Acc:0.7735\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 07 Valid Loss:0.0705 Acc:0.6000\n",
            "  P:0.5917 R:0.6150 F1:0.5427\n",
            "  CM:\n",
            " [[118  60   0   0   1]\n",
            " [  0  34   3   0   0]\n",
            " [  0  38  39  15   8]\n",
            " [  0   1   3   9   6]\n",
            " [  0   6   5   0  19]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [00:26<00:00,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance ===\n",
            "Test Loss : 0.0561\n",
            "Test Acc  : 0.8192\n",
            "Precision : 0.6984\n",
            "Recall    : 0.6878\n",
            "F1-score  : 0.6770\n",
            "Confusion Matrix:\n",
            "[[179   1   0   0   0]\n",
            " [  2  22  12   0   1]\n",
            " [  1   8  71  19   0]\n",
            " [  0   1   2  13   4]\n",
            " [  0   1   7   7  14]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TqdpTdpKFzz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''**** (추가 !!!!!!!) APTOS 데이터만 사용, StratifiedShuffleSplit을 두 단계로 사용해 전체 APTOS를 80/10/10으로 나눴고,\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만 거치게 됩니다.\n",
        "WeightedRandomsampler 추가\n",
        "나머지 학습 루프(optimizer, scheduler, loss)는 기본 shuffle + nn.CrossEntropyLoss() 세팅 그대로\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# Colab에서 실행 시 필요한 라이브러리 설치\n",
        "# !pip install opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (On-DAT 클래스별 증강)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 파일 존재 필터링\n",
        "# ----------------------------------------\n",
        "ROOT    = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path= os.path.join(ROOT, 'train.csv')\n",
        "img_dir = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image']\n",
        "    .apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10)\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = 1.0 / class_counts\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler  = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (Sampler 적용)\n",
        "# ----------------------------------------\n",
        "train_ds     = APTOSDataset(df_train, img_dir)\n",
        "val_ds       = APTOSDataset(df_val,   img_dir)\n",
        "test_ds      = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", counts)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc; no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos_aug.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos_aug.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Hz07wCYLsFb",
        "outputId": "edb736af-2ea9-4c75-b480-f040fa701a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-380291817>:186: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-8-380291817>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 1: 100%|██████████| 183/183 [03:22<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 0.8893, Acc: 0.6158\n",
            "  Class counts: [606, 571, 567, 603, 571]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.4689, Acc: 0.8384\n",
            "  P: 0.7465, R: 0.7540, F1: 0.7208\n",
            "  CM:\n",
            " [[169   9   1   0   0]\n",
            " [  1  34   2   0   0]\n",
            " [  0   9  77  13   1]\n",
            " [  0   0   4  14   1]\n",
            " [  0   1  12   5  12]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-8-380291817>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [03:24<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.7185, Acc: 0.7193\n",
            "  Class counts: [567, 599, 594, 569, 589]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 0.6557, Acc: 0.7315\n",
            "  P: 0.6569, R: 0.6560, F1: 0.5900\n",
            "  CM:\n",
            " [[172   6   1   0   0]\n",
            " [  1  28   8   0   0]\n",
            " [  1  11  42  44   2]\n",
            " [  0   0   2  16   1]\n",
            " [  0   1   5  15   9]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-8-380291817>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 0.8859, Acc: 0.6419\n",
            "  Class counts: [610, 604, 517, 597, 590]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 0.6796, Acc: 0.6986\n",
            "  P: 0.5729, R: 0.6041, F1: 0.5558\n",
            "  CM:\n",
            " [[169   6   4   0   0]\n",
            " [  2  19  15   1   0]\n",
            " [  0   2  37  20  41]\n",
            " [  0   0   0  10   9]\n",
            " [  0   0   4   6  20]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-8-380291817>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:24<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 0.8073, Acc: 0.6659\n",
            "  Class counts: [606, 587, 576, 583, 566]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 0.8012, Acc: 0.7123\n",
            "  P: 0.6030, R: 0.6373, F1: 0.5780\n",
            "  CM:\n",
            " [[143  36   0   0   0]\n",
            " [  1  33   3   0   0]\n",
            " [  0  19  65  14   2]\n",
            " [  0   0   3  11   5]\n",
            " [  0   3  10   9   8]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-8-380291817>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:26<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 1.5387, Acc: 0.2728\n",
            "  Class counts: [588, 585, 614, 572, 559]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 1.5268, Acc: 0.4904\n",
            "  P: 0.0981, R: 0.2000, F1: 0.1316\n",
            "  CM:\n",
            " [[179   0   0   0   0]\n",
            " [ 37   0   0   0   0]\n",
            " [100   0   0   0   0]\n",
            " [ 19   0   0   0   0]\n",
            " [ 30   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-8-380291817>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:25<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 1.6211, Acc: 0.2005\n",
            "  Class counts: [591, 563, 561, 608, 595]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 1.6237, Acc: 0.0521\n",
            "  P: 0.0104, R: 0.2000, F1: 0.0198\n",
            "  CM:\n",
            " [[  0   0   0 179   0]\n",
            " [  0   0   0  37   0]\n",
            " [  0   0   0 100   0]\n",
            " [  0   0   0  19   0]\n",
            " [  0   0   0  30   0]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [00:26<00:00,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance ===\n",
            "Test Loss : 0.4118\n",
            "Test Acc  : 0.8767\n",
            "Precision : 0.7439\n",
            "Recall    : 0.7571\n",
            "F1-score  : 0.7466\n",
            "Confusion Matrix:\n",
            "[[179   1   0   0   0]\n",
            " [  0  34   3   0   0]\n",
            " [  0  12  79   6   2]\n",
            " [  0   0   7   7   6]\n",
            " [  0   1   4   3  21]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''**** 재탕 APTOS 데이터만 사용, StratifiedShuffleSplit을 두 단계로 사용해 전체 APTOS를 80/10/10으로 나눴고,\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만 거치게 됩니다.\n",
        "WeightedRandomsampler 추가\n",
        "나머지 학습 루프(optimizer, scheduler, loss)는 기본 shuffle + nn.CrossEntropyLoss() 세팅 그대로\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# Colab에서 실행 시 필요한 라이브러리 설치\n",
        "# !pip install opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (On-DAT 클래스별 증강)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 파일 존재 필터링\n",
        "# ----------------------------------------\n",
        "ROOT    = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path= os.path.join(ROOT, 'train.csv')\n",
        "img_dir = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image']\n",
        "    .apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10)\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = 1.0 / class_counts\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler  = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (Sampler 적용)\n",
        "# ----------------------------------------\n",
        "train_ds     = APTOSDataset(df_train, img_dir)\n",
        "val_ds       = APTOSDataset(df_val,   img_dir)\n",
        "test_ds      = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", counts)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc; no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos_aug.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos_aug.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQXcsKVRkK0f",
        "outputId": "012d6780-fa2d-4e99-d3d8-087d16c87705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-1606037854>:186: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-11-1606037854>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 1: 100%|██████████| 183/183 [03:25<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 0.8335, Acc: 0.6559\n",
            "  Class counts: [555, 566, 568, 612, 617]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.4387, Acc: 0.8548\n",
            "  P: 0.7420, R: 0.7542, F1: 0.7404\n",
            "  CM:\n",
            " [[175   4   0   0   0]\n",
            " [  2  33   2   0   0]\n",
            " [  1   8  77   9   5]\n",
            " [  0   1   5  12   1]\n",
            " [  0   1  11   3  15]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-11-1606037854>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [03:24<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.6713, Acc: 0.7395\n",
            "  Class counts: [570, 578, 573, 555, 642]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 0.6077, Acc: 0.7288\n",
            "  P: 0.7109, R: 0.6383, F1: 0.5679\n",
            "  CM:\n",
            " [[172   7   0   0   0]\n",
            " [  4  20  11   2   0]\n",
            " [  2   4  49  45   0]\n",
            " [  0   0   0  19   0]\n",
            " [  0   1   7  16   6]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-11-1606037854>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:24<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 0.8535, Acc: 0.6343\n",
            "  Class counts: [560, 546, 609, 607, 596]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:24<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 0.7694, Acc: 0.7808\n",
            "  P: 0.6467, R: 0.6017, F1: 0.6159\n",
            "  CM:\n",
            " [[178   1   0   0   0]\n",
            " [ 18  16   3   0   0]\n",
            " [  8   5  68   9  10]\n",
            " [  0   1   5   7   6]\n",
            " [  3   0   8   3  16]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-11-1606037854>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:27<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 1.1510, Acc: 0.4983\n",
            "  Class counts: [544, 584, 639, 589, 562]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 1.5804, Acc: 0.2411\n",
            "  P: 0.1667, R: 0.2212, F1: 0.1510\n",
            "  CM:\n",
            " [[ 53  15 111   0   0]\n",
            " [  5  27   5   0   0]\n",
            " [ 23  69   8   0   0]\n",
            " [  7  12   0   0   0]\n",
            " [  2  26   2   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-11-1606037854>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:25<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 1.5989, Acc: 0.2574\n",
            "  Class counts: [550, 598, 611, 593, 566]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 1.4606, Acc: 0.5479\n",
            "  P: 0.1842, R: 0.3307, F1: 0.2293\n",
            "  CM:\n",
            " [[175   4   0   0   0]\n",
            " [ 12  25   0   0   0]\n",
            " [ 43  56   0   0   1]\n",
            " [ 10   9   0   0   0]\n",
            " [  8  22   0   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-11-1606037854>:216: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:20<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 1.5574, Acc: 0.3283\n",
            "  Class counts: [622, 615, 558, 565, 558]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:24<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 1.3979, Acc: 0.5151\n",
            "  P: 0.2779, R: 0.3302, F1: 0.2383\n",
            "  CM:\n",
            " [[156  19   4   0   0]\n",
            " [ 10  27   0   0   0]\n",
            " [ 32  63   5   0   0]\n",
            " [  5  12   2   0   0]\n",
            " [  5  25   0   0   0]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [00:26<00:00,  1.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance ===\n",
            "Test Loss : 0.3698\n",
            "Test Acc  : 0.8575\n",
            "Precision : 0.7097\n",
            "Recall    : 0.7378\n",
            "F1-score  : 0.7201\n",
            "Confusion Matrix:\n",
            "[[179   1   0   0   0]\n",
            " [  1  33   2   0   1]\n",
            " [  1  10  74   9   5]\n",
            " [  0   1   5   8   6]\n",
            " [  0   2   3   5  19]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''**** seed 고정 APTOS 데이터만 사용, StratifiedShuffleSplit을 두 단계로 사용해 전체 APTOS를 80/10/10으로 나눴고,\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만 거치게 됩니다.\n",
        "WeightedRandomsampler 추가\n",
        "나머지 학습 루프(optimizer, scheduler, loss)는 기본 shuffle + nn.CrossEntropyLoss() 세팅 그대로\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# -------------------------------\n",
        "# 실험 재현성 위한 Seed 고정 함수 추가\n",
        "# -------------------------------\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)  # 코드 제일 앞에서 실행!\n",
        "\n",
        "# -------------------------------\n",
        "# 이하 기존 코드\n",
        "# -------------------------------\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (On-DAT 클래스별 증강)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "\n",
        "        # class2, 3, 4만 증강, 나머지는 base transform만 적용\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 파일 존재 필터링\n",
        "# ----------------------------------------\n",
        "ROOT    = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path= os.path.join(ROOT, 'train.csv')\n",
        "img_dir = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image']\n",
        "    .apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10) - random_state 고정!\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = 1.0 / class_counts\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler  = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (Sampler 적용)\n",
        "# ----------------------------------------\n",
        "train_ds     = APTOSDataset(df_train, img_dir)\n",
        "val_ds       = APTOSDataset(df_val,   img_dir)\n",
        "test_ds      = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", counts)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc; no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos_aug_2.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos_aug_2.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b6413ce0457a424d815810d49646500a",
            "bfaede04cda04fd4b64ffcd716f4e350",
            "3a10fcb5c67349c2aaa5daa7f08083df",
            "bd69ba617c484d0cb971eb7d6f4946b0",
            "7ab7e3dbfa474f87bb04fd45815dc487",
            "34f83e1e081d4b408b55f37efbf2d220",
            "f2ba324fc1564df18b5d64b264ec20fc",
            "4e27bba8a85d437197857d4cc5e8254f",
            "decf1e33f61a4c7db13afb479bc9d30b",
            "4ec723b7b5a24c5ca75bca1b44d57f11",
            "5657dca73d4948eca41e9a871adb9db6"
          ]
        },
        "id": "8GvB9A4CvQuZ",
        "outputId": "a97c420d-7b8f-4c05-c9c8-605413e45068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/801M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6413ce0457a424d815810d49646500a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-4101348122>:201: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-4101348122>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 1: 100%|██████████| 183/183 [06:03<00:00,  1.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 0.8756, Acc: 0.6364\n",
            "  Class counts: [554, 583, 616, 584, 581]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [01:04<00:00,  2.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.5249, Acc: 0.7863\n",
            "  P: 0.6954, R: 0.7232, F1: 0.6578\n",
            "  CM:\n",
            " [[171   8   0   0   0]\n",
            " [  0  33   4   0   0]\n",
            " [  1   8  56  32   3]\n",
            " [  0   0   2  16   1]\n",
            " [  0   2   5  12  11]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-4101348122>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [04:25<00:00,  1.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.6594, Acc: 0.7341\n",
            "  Class counts: [595, 604, 578, 578, 563]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 0.6391, Acc: 0.7260\n",
            "  P: 0.6566, R: 0.6974, F1: 0.6138\n",
            "  CM:\n",
            " [[159  20   0   0   0]\n",
            " [  0  35   2   0   0]\n",
            " [  1   9  43  43   4]\n",
            " [  0   0   3  15   1]\n",
            " [  0   1   5  11  13]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-4101348122>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:58<00:00,  1.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 0.8150, Acc: 0.6782\n",
            "  Class counts: [582, 577, 562, 612, 585]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 0.7267, Acc: 0.6630\n",
            "  P: 0.5850, R: 0.5979, F1: 0.5202\n",
            "  CM:\n",
            " [[162  15   0   0   2]\n",
            " [  3  28   0   3   3]\n",
            " [  1   4  24  21  50]\n",
            " [  0   0   1   8  10]\n",
            " [  0   0   1   9  20]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-4101348122>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:46<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 1.3832, Acc: 0.3537\n",
            "  Class counts: [568, 554, 607, 588, 601]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 1.4001, Acc: 0.4904\n",
            "  P: 0.0984, R: 0.2000, F1: 0.1319\n",
            "  CM:\n",
            " [[179   0   0   0   0]\n",
            " [ 37   0   0   0   0]\n",
            " [ 99   0   0   0   1]\n",
            " [ 19   0   0   0   0]\n",
            " [ 30   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-4101348122>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:45<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 1.6484, Acc: 0.1988\n",
            "  Class counts: [597, 565, 578, 580, 598]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 1.6210, Acc: 0.0822\n",
            "  P: 0.0164, R: 0.2000, F1: 0.0304\n",
            "  CM:\n",
            " [[  0   0   0   0 179]\n",
            " [  0   0   0   0  37]\n",
            " [  0   0   0   0 100]\n",
            " [  0   0   0   0  19]\n",
            " [  0   0   0   0  30]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-4101348122>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:36<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 1.6188, Acc: 0.2019\n",
            "  Class counts: [580, 572, 570, 582, 614]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 1.6634, Acc: 0.0822\n",
            "  P: 0.0164, R: 0.2000, F1: 0.0304\n",
            "  CM:\n",
            " [[  0   0   0   0 179]\n",
            " [  0   0   0   0  37]\n",
            " [  0   0   0   0 100]\n",
            " [  0   0   0   0  19]\n",
            " [  0   0   0   0  30]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [01:19<00:00,  3.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance ===\n",
            "Test Loss : 0.4875\n",
            "Test Acc  : 0.8247\n",
            "Precision : 0.7153\n",
            "Recall    : 0.7519\n",
            "F1-score  : 0.7097\n",
            "Confusion Matrix:\n",
            "[[175   5   0   0   0]\n",
            " [  0  32   5   0   0]\n",
            " [  0  14  63  20   2]\n",
            " [  0   1   2  14   3]\n",
            " [  0   0   3   9  17]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''**** (재탕 !!!!) seed 고정 APTOS 데이터만 사용, StratifiedShuffleSplit을 두 단계로 사용해 전체 APTOS를 80/10/10으로 나눴고,\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만 거치게 됩니다.\n",
        "WeightedRandomsampler 추가\n",
        "나머지 학습 루프(optimizer, scheduler, loss)는 기본 shuffle + nn.CrossEntropyLoss() 세팅 그대로\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# -------------------------------\n",
        "# 실험 재현성 위한 Seed 고정 함수 추가\n",
        "# -------------------------------\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)  # 코드 제일 앞에서 실행!\n",
        "\n",
        "# -------------------------------\n",
        "# 이하 기존 코드\n",
        "# -------------------------------\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (On-DAT 클래스별 증강)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "\n",
        "        # class2, 3, 4만 증강, 나머지는 base transform만 적용\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 파일 존재 필터링\n",
        "# ----------------------------------------\n",
        "ROOT    = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path= os.path.join(ROOT, 'train.csv')\n",
        "img_dir = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image']\n",
        "    .apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10) - random_state 고정!\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = 1.0 / class_counts\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler  = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (Sampler 적용)\n",
        "# ----------------------------------------\n",
        "train_ds     = APTOSDataset(df_train, img_dir)\n",
        "val_ds       = APTOSDataset(df_val,   img_dir)\n",
        "test_ds      = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", counts)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc; no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos_aug_3.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos_aug_3.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z32q66Owejh6",
        "outputId": "22b031e7-bc04-4af7-fde6-5a9d308b76d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-1320424030>:201: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-1320424030>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 1: 100%|██████████| 183/183 [03:25<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 0.8716, Acc: 0.6395\n",
            "  Class counts: [554, 583, 616, 584, 581]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.7366, Acc: 0.7014\n",
            "  P: 0.6534, R: 0.6350, F1: 0.5629\n",
            "  CM:\n",
            " [[178   1   0   0   0]\n",
            " [ 12  23   1   1   0]\n",
            " [  6  17  25  49   3]\n",
            " [  0   1   1  16   1]\n",
            " [  0   3   2  11  14]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-1320424030>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [03:21<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.6572, Acc: 0.7474\n",
            "  Class counts: [595, 604, 578, 578, 563]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 0.4716, Acc: 0.8247\n",
            "  P: 0.6998, R: 0.7487, F1: 0.7172\n",
            "  CM:\n",
            " [[164  15   0   0   0]\n",
            " [  0  32   5   0   0]\n",
            " [  0   7  75   6  12]\n",
            " [  0   0   4  11   4]\n",
            " [  0   1   7   3  19]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-1320424030>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:24<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 0.7734, Acc: 0.6981\n",
            "  Class counts: [582, 577, 562, 612, 585]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 0.7525, Acc: 0.6904\n",
            "  P: 0.5939, R: 0.6685, F1: 0.5659\n",
            "  CM:\n",
            " [[165  13   0   0   1]\n",
            " [  5  27   4   0   1]\n",
            " [  5   7  24  44  20]\n",
            " [  0   0   0  13   6]\n",
            " [  0   1   2   4  23]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-1320424030>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:26<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 0.8501, Acc: 0.6672\n",
            "  Class counts: [568, 554, 607, 588, 601]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 2.4428, Acc: 0.1945\n",
            "  P: 0.3147, R: 0.2741, F1: 0.1474\n",
            "  CM:\n",
            " [[ 1 41 60 77  0]\n",
            " [ 0  9 17 11  0]\n",
            " [ 0  6 49 45  0]\n",
            " [ 0  0  7 12  0]\n",
            " [ 0  1 10 19  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-1320424030>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:26<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 0.8913, Acc: 0.6374\n",
            "  Class counts: [597, 565, 578, 580, 598]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 0.5969, Acc: 0.7726\n",
            "  P: 0.6866, R: 0.6459, F1: 0.6133\n",
            "  CM:\n",
            " [[172   7   0   0   0]\n",
            " [  6  29   2   0   0]\n",
            " [  6  11  62  21   0]\n",
            " [  0   1   5  12   1]\n",
            " [  0   3  14   6   7]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-1320424030>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:25<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 0.7511, Acc: 0.6947\n",
            "  Class counts: [580, 572, 570, 582, 614]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 1.7454, Acc: 0.4192\n",
            "  P: 0.5631, R: 0.3875, F1: 0.3178\n",
            "  CM:\n",
            " [[102   0   2  15  60]\n",
            " [  2   7   3   5  20]\n",
            " [  0   0  14  14  72]\n",
            " [  0   0   1   2  16]\n",
            " [  0   0   2   0  28]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 7:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-1320424030>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 7: 100%|██████████| 183/183 [03:32<00:00,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Train Loss: 0.8140, Acc: 0.6614\n",
            "  Class counts: [593, 555, 604, 547, 619]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Valid Loss: 0.8635, Acc: 0.7288\n",
            "  P: 0.6266, R: 0.5730, F1: 0.5855\n",
            "  CM:\n",
            " [[173   5   1   0   0]\n",
            " [ 12  22   3   0   0]\n",
            " [ 25   7  53  11   4]\n",
            " [  2   1   5   9   2]\n",
            " [  5   3  13   0   9]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [00:26<00:00,  1.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance ===\n",
            "Test Loss : 0.3906\n",
            "Test Acc  : 0.8603\n",
            "Precision : 0.7506\n",
            "Recall    : 0.7507\n",
            "F1-score  : 0.7283\n",
            "Confusion Matrix:\n",
            "[[173   7   0   0   0]\n",
            " [  0  35   1   0   1]\n",
            " [  0   6  78   2  13]\n",
            " [  0   0   6   6   8]\n",
            " [  0   0   6   1  22]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''**** (재탕 !!!!) seed 고정 APTOS 데이터만 사용, StratifiedShuffleSplit을 두 단계로 사용해 전체 APTOS를 80/10/10으로 나눴고,\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만 거치게 됩니다.\n",
        "WeightedRandomsampler 추가\n",
        "손실 함수를 nn.CrossEntropyLoss() 대신 Focal Loss로 교체\n",
        "나머지 학습 루프(optimizer, scheduler, loss)는 기본 shuffle + nn.CrossEntropyLoss() 세팅 그대로\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# -------------------------------\n",
        "# 실험 재현성 위한 Seed 고정 함수 추가\n",
        "# -------------------------------\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)  # 코드 제일 앞에서 실행!\n",
        "\n",
        "# -------------------------------\n",
        "# 이하 기존 코드 + Focal Loss 정의\n",
        "# -------------------------------\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# Focal Loss 구현\n",
        "# ----------------------------------------\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma: float = 2.0, alpha: float = 0.25, reduction: str = \"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.reduction = reduction\n",
        "        self.ce = nn.CrossEntropyLoss(reduction=\"none\")\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        logpt = -self.ce(inputs, targets)\n",
        "        pt = torch.exp(logpt)\n",
        "        focal_term = (1 - pt) ** self.gamma\n",
        "        loss = -self.alpha * focal_term * logpt\n",
        "        if self.reduction == \"mean\":\n",
        "            return loss.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return loss.sum()\n",
        "        else:\n",
        "            return loss\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (On-DAT 클래스별 증강)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "\n",
        "        # class2, 3, 4만 증강, 나머지는 base transform만 적용\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 파일 존재 필터링\n",
        "# ----------------------------------------\n",
        "ROOT    = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path= os.path.join(ROOT, 'train.csv')\n",
        "img_dir = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image']\n",
        "    .apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10) - random_state 고정!\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = 1.0 / class_counts\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler  = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (Sampler 적용)\n",
        "# ----------------------------------------\n",
        "train_ds     = APTOSDataset(df_train, img_dir)\n",
        "val_ds       = APTOSDataset(df_val,   img_dir)\n",
        "test_ds      = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = FocalLoss(gamma=2.0, alpha=0.25, reduction=\"mean\")\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", counts)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc; no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos_focal.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos_focal.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e371c5590dae403f913d3c56d464d2c1",
            "a877ee1afca04754aefcb82d237538a4",
            "cdaca1c3b30d4510859b2f10ed73c01d",
            "fee563515bb74b76b3744bec1769780b",
            "621e9657ae4740519cff17b2c236a397",
            "f96da5e5a9ac43fab6e98e32164c9597",
            "0eb5f878d3c94406a4a0e53b893e447a",
            "34cd1776d06c45b7a936a32b6a370f01",
            "8f7cfc111d434341a96ae0a94cbc702e",
            "b76eaceb1da3481f8d5d594d72015579",
            "f363609dfeb5450bbee4253f9b664a8a"
          ]
        },
        "id": "ee0sBPiG2HIA",
        "outputId": "f4df934a-8880-4f6d-88f4-9874b2ea1af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/801M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e371c5590dae403f913d3c56d464d2c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2649669442>:224: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2649669442>:253: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 1: 100%|██████████| 183/183 [05:53<00:00,  1.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 0.1071, Acc: 0.6501\n",
            "  Class counts: [554, 583, 616, 584, 581]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [01:06<00:00,  2.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.0742, Acc: 0.7315\n",
            "  P: 0.6865, R: 0.6140, F1: 0.5687\n",
            "  CM:\n",
            " [[179   0   0   0   0]\n",
            " [ 13  15   9   0   0]\n",
            " [  2   9  47  41   1]\n",
            " [  0   0   2  17   0]\n",
            " [  0   0   4  17   9]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2649669442>:253: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [04:40<00:00,  1.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.0786, Acc: 0.7330\n",
            "  Class counts: [595, 604, 578, 578, 563]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 0.0775, Acc: 0.7014\n",
            "  P: 0.6681, R: 0.6169, F1: 0.5147\n",
            "  CM:\n",
            " [[175   4   0   0   0]\n",
            " [  4  32   0   1   0]\n",
            " [  2  19  30  48   1]\n",
            " [  0   1   2  16   0]\n",
            " [  0   1   2  24   3]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2649669442>:253: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:53<00:00,  1.27s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 0.0867, Acc: 0.7077\n",
            "  Class counts: [582, 577, 562, 612, 585]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 0.1536, Acc: 0.5479\n",
            "  P: 0.3734, R: 0.5297, F1: 0.3935\n",
            "  CM:\n",
            " [[153  23   0   1   2]\n",
            " [  3  16   0  11   7]\n",
            " [  3   4   0  83  10]\n",
            " [  0   0   0  17   2]\n",
            " [  0   0   0  16  14]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2649669442>:253: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:51<00:00,  1.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 0.1226, Acc: 0.6080\n",
            "  Class counts: [568, 554, 607, 588, 601]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 0.1256, Acc: 0.5562\n",
            "  P: 0.6416, R: 0.4843, F1: 0.3969\n",
            "  CM:\n",
            " [[130  48   0   0   1]\n",
            " [  2  35   0   0   0]\n",
            " [  2  35  23   0  40]\n",
            " [  0   4   1   1  13]\n",
            " [  0  10   6   0  14]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2649669442>:253: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:45<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 0.2551, Acc: 0.2481\n",
            "  Class counts: [597, 565, 578, 580, 598]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:24<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 0.2281, Acc: 0.4904\n",
            "  P: 0.2217, R: 0.2933, F1: 0.2400\n",
            "  CM:\n",
            " [[159  19   0   0   1]\n",
            " [ 23  14   0   0   0]\n",
            " [ 51  39   0   0  10]\n",
            " [ 12   6   0   0   1]\n",
            " [ 11  13   0   0   6]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-3-2649669442>:253: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:38<00:00,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 0.2570, Acc: 0.2186\n",
            "  Class counts: [580, 572, 570, 582, 614]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 0.2826, Acc: 0.0822\n",
            "  P: 0.0164, R: 0.2000, F1: 0.0304\n",
            "  CM:\n",
            " [[  0   0   0   0 179]\n",
            " [  0   0   0   0  37]\n",
            " [  0   0   0   0 100]\n",
            " [  0   0   0   0  19]\n",
            " [  0   0   0   0  30]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [01:03<00:00,  2.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance ===\n",
            "Test Loss : 0.0659\n",
            "Test Acc  : 0.7342\n",
            "Precision : 0.6768\n",
            "Recall    : 0.6157\n",
            "F1-score  : 0.5717\n",
            "Confusion Matrix:\n",
            "[[180   0   0   0   0]\n",
            " [ 14  12  11   0   0]\n",
            " [  1   3  47  44   4]\n",
            " [  0   0   0  18   2]\n",
            " [  0   0   2  16  11]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''EfficientNet-B3\n",
        "seed 고정 APTOS 데이터만 사용\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만 거치게 됩니다.\n",
        "WeightedRandomsampler 사용\n",
        "preprocess_size를 (300, 300)\n",
        "나머지 학습 루프(optimizer, scheduler, loss)는 기본 shuffle + nn.CrossEntropyLoss() 세팅 그대로\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# -------------------------------\n",
        "# 실험 재현성 위한 Seed 고정 함수 추가\n",
        "# -------------------------------\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)  # 코드 제일 앞에서 실행!\n",
        "\n",
        "# -------------------------------\n",
        "# 이하 기존 코드 (변경된 부분만 주석 처리)\n",
        "# -------------------------------\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(300, 300)):  # 384→300\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (On-DAT 클래스별 증강)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(300,300)):  # 384→300\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "\n",
        "        # class2, 3, 4만 증강, 나머지는 base transform만 적용\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 파일 존재 필터링\n",
        "# ----------------------------------------\n",
        "ROOT     = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path = os.path.join(ROOT, 'train.csv')\n",
        "img_dir  = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image']\n",
        "    .apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10) - random_state 고정!\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = 1.0 / class_counts\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler  = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (Sampler 적용)\n",
        "# ----------------------------------------\n",
        "train_ds     = APTOSDataset(df_train, img_dir)\n",
        "val_ds       = APTOSDataset(df_val,   img_dir)\n",
        "test_ds      = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'efficientnet_b3',            # Swin → EfficientNet-B3\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", counts)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc; no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_efficientnetb3_aptos.pth')\n",
        "        print(\">> Best EfficientNet-B3 model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'efficientnet_b3',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_efficientnetb3_aptos.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance (EfficientNet-B3) ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b3c1204d4074404da883b86224221a68",
            "da23dafff5b744d0b82778234f571695",
            "abfb456bfb4a4484aacac61720c8f5bc",
            "b3eb1e4c8241429e912c54cedcf5b625",
            "478cb325deb3464aa632220a58b56421",
            "0d6e3a91db3446fba374acb588fb88ff",
            "4bb04587d4d14222b292b4ff0f1ff824",
            "990d7ce5f0304d4fbdb8e3649563b461",
            "f2d7fea344ea4c07b58b1905240eab5b",
            "c090408547d745aeb7956d047e7602da",
            "9b4b64cd88004e0ab396aaadb50457cd"
          ]
        },
        "id": "JPM7ne28-h2B",
        "outputId": "a56ed4e9-340b-4b14-d815-ab26e1663501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3c1204d4074404da883b86224221a68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3291045879>:202: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "Train Epoch 1: 100%|██████████| 183/183 [03:40<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 1.2037, Acc: 0.5894\n",
            "  Class counts: [584, 601, 581, 574, 578]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.8082, Acc: 0.7425\n",
            "  P: 0.5969, R: 0.6343, F1: 0.6001\n",
            "  CM:\n",
            " [[166   9   3   1   0]\n",
            " [  3  25   6   2   1]\n",
            " [  7   8  57  20   8]\n",
            " [  0   0   4  12   3]\n",
            " [  0   2  12   5  11]]\n",
            ">> Best EfficientNet-B3 model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [03:42<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.5631, Acc: 0.8112\n",
            "  Class counts: [585, 592, 551, 618, 572]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:24<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 1.0666, Acc: 0.6795\n",
            "  P: 0.5755, R: 0.6257, F1: 0.5664\n",
            "  CM:\n",
            " [[151  26   2   0   0]\n",
            " [  2  23  12   0   0]\n",
            " [  4   8  46  35   7]\n",
            " [  0   0   4  14   1]\n",
            " [  0   2   9   5  14]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:47<00:00,  1.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 0.5481, Acc: 0.8249\n",
            "  Class counts: [539, 612, 615, 523, 629]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:24<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 0.9191, Acc: 0.7781\n",
            "  P: 0.6642, R: 0.6128, F1: 0.6132\n",
            "  CM:\n",
            " [[173   5   1   0   0]\n",
            " [ 10  17   9   0   1]\n",
            " [  9   2  74  12   3]\n",
            " [  0   0   6  12   1]\n",
            " [  2   0  13   7   8]]\n",
            ">> Best EfficientNet-B3 model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:38<00:00,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 0.4864, Acc: 0.8437\n",
            "  Class counts: [609, 578, 589, 570, 572]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 0.7046, Acc: 0.7973\n",
            "  P: 0.6749, R: 0.7016, F1: 0.6796\n",
            "  CM:\n",
            " [[172   5   0   0   2]\n",
            " [  6  22   6   0   3]\n",
            " [  5   6  64   9  16]\n",
            " [  0   0   4  11   4]\n",
            " [  0   0   6   2  22]]\n",
            ">> Best EfficientNet-B3 model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:40<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 0.3637, Acc: 0.8807\n",
            "  Class counts: [579, 595, 568, 566, 610]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 0.6732, Acc: 0.8164\n",
            "  P: 0.6805, R: 0.6807, F1: 0.6741\n",
            "  CM:\n",
            " [[174   5   0   0   0]\n",
            " [  2  26   8   0   1]\n",
            " [  2   7  75  11   5]\n",
            " [  0   0   6  11   2]\n",
            " [  0   1  13   4  12]]\n",
            ">> Best EfficientNet-B3 model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:32<00:00,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 0.2993, Acc: 0.9023\n",
            "  Class counts: [527, 596, 610, 602, 583]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 0.8998, Acc: 0.7534\n",
            "  P: 0.6112, R: 0.6661, F1: 0.6266\n",
            "  CM:\n",
            " [[150  28   0   0   1]\n",
            " [  2  29   4   0   2]\n",
            " [  2   4  73   9  12]\n",
            " [  0   0   6  11   2]\n",
            " [  0   2  10   6  12]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 7:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 7: 100%|██████████| 183/183 [03:40<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Train Loss: 0.2111, Acc: 0.9291\n",
            "  Class counts: [569, 604, 574, 596, 575]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Valid Loss: 0.5687, Acc: 0.8493\n",
            "  P: 0.7355, R: 0.7060, F1: 0.7106\n",
            "  CM:\n",
            " [[174   4   1   0   0]\n",
            " [  2  30   5   0   0]\n",
            " [  1   5  84   7   3]\n",
            " [  0   0   9   9   1]\n",
            " [  0   1  10   6  13]]\n",
            ">> Best EfficientNet-B3 model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 8:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 8: 100%|██████████| 183/183 [03:37<00:00,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 008 — Train Loss: 0.2019, Acc: 0.9335\n",
            "  Class counts: [562, 570, 588, 589, 609]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 008 — Valid Loss: 0.7479, Acc: 0.8247\n",
            "  P: 0.6995, R: 0.7081, F1: 0.7012\n",
            "  CM:\n",
            " [[169  10   0   0   0]\n",
            " [  3  23  11   0   0]\n",
            " [  2   4  81   5   8]\n",
            " [  0   0   6  12   1]\n",
            " [  1   1   5   7  16]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 9:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 9: 100%|██████████| 183/183 [03:40<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 009 — Train Loss: 0.2066, Acc: 0.9304\n",
            "  Class counts: [558, 565, 643, 563, 589]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 009 — Valid Loss: 0.7665, Acc: 0.8274\n",
            "  P: 0.7306, R: 0.6639, F1: 0.6879\n",
            "  CM:\n",
            " [[176   3   0   0   0]\n",
            " [  6  19  12   0   0]\n",
            " [  3   3  83   6   5]\n",
            " [  0   0   8  10   1]\n",
            " [  0   1  13   2  14]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 10:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 10: 100%|██████████| 183/183 [03:34<00:00,  1.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 010 — Train Loss: 0.2137, Acc: 0.9287\n",
            "  Class counts: [570, 595, 578, 582, 593]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 010 — Valid Loss: 0.6356, Acc: 0.8438\n",
            "  P: 0.7217, R: 0.7338, F1: 0.7266\n",
            "  CM:\n",
            " [[173   6   0   0   0]\n",
            " [  2  27   7   0   1]\n",
            " [  1   4  78   7  10]\n",
            " [  0   0   4  10   5]\n",
            " [  0   1   8   1  20]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 11:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 11: 100%|██████████| 183/183 [03:35<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 011 — Train Loss: 0.1824, Acc: 0.9400\n",
            "  Class counts: [592, 560, 586, 588, 592]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 011 — Valid Loss: 0.7320, Acc: 0.8329\n",
            "  P: 0.7386, R: 0.6599, F1: 0.6841\n",
            "  CM:\n",
            " [[174   5   0   0   0]\n",
            " [  4  25   8   0   0]\n",
            " [  2   6  85   4   3]\n",
            " [  0   0  11   7   1]\n",
            " [  0   2  14   1  13]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 12:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3291045879>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 12: 100%|██████████| 183/183 [03:36<00:00,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 012 — Train Loss: 0.1432, Acc: 0.9548\n",
            "  Class counts: [571, 615, 571, 598, 563]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 012 — Valid Loss: 0.7989, Acc: 0.7973\n",
            "  P: 0.6888, R: 0.7096, F1: 0.6818\n",
            "  CM:\n",
            " [[174   5   0   0   0]\n",
            " [  6  21   9   0   1]\n",
            " [  2   0  61   9  28]\n",
            " [  0   0   4  12   3]\n",
            " [  0   1   3   3  23]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [00:24<00:00,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance (EfficientNet-B3) ===\n",
            "Test Loss : 0.5490\n",
            "Test Acc  : 0.8630\n",
            "Precision : 0.6960\n",
            "Recall    : 0.6858\n",
            "F1-score  : 0.6805\n",
            "Confusion Matrix:\n",
            "[[179   1   0   0   0]\n",
            " [  1  34   2   0   0]\n",
            " [  1   9  84   4   1]\n",
            " [  0   3   8   3   6]\n",
            " [  0   2   9   3  15]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''DenseNet-121\n",
        "seed 고정 APTOS 데이터만 사용\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만 거치게 됩니다.\n",
        "WeightedRandomsampler 사용\n",
        "preprocess_size를 (224, 224)\n",
        "나머지 학습 루프(optimizer, scheduler, loss)는 기본 shuffle + nn.CrossEntropyLoss() 세팅 그대로\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# -------------------------------\n",
        "# 실험 재현성 위한 Seed 고정 함수 추가\n",
        "# -------------------------------\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)  # 코드 제일 앞에서 실행!\n",
        "\n",
        "# -------------------------------\n",
        "# 이하 기존 코드 (변경된 부분만 주석 처리)\n",
        "# -------------------------------\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(224, 224)):  # 300→224\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (On-DAT 클래스별 증강)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(224,224)):  # 300→224\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "\n",
        "        # class2, 3, 4만 증강, 나머지는 base transform만 적용\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 파일 존재 필터링\n",
        "# ----------------------------------------\n",
        "ROOT     = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path = os.path.join(ROOT, 'train.csv')\n",
        "img_dir  = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image']\n",
        "    .apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10) - random_state 고정!\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = 1.0 / class_counts\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler  = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (Sampler 적용)\n",
        "# ----------------------------------------\n",
        "train_ds     = APTOSDataset(df_train, img_dir)\n",
        "val_ds       = APTOSDataset(df_val,   img_dir)\n",
        "test_ds      = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'densenet121',            # EfficientNet → DenseNet-121\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", counts)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc; no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_densenet121_aptos.pth')\n",
        "        print(\">> Best DenseNet-121 model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'densenet121',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_densenet121_aptos.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance (DenseNet-121) ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4bff2d2cf3ea47339287e5509d320526",
            "1d066c906eda485fa89e730d2c964faa",
            "a08e103c04d2409e9a43f992ca05da8f",
            "67aa03d3050a4af8988c6cc38a60ed66",
            "bd5f8728ca194c6383efdc730a2dbd2a",
            "cecd21ca1d554da69b7810d93519a7df",
            "afd371f312f740149fb6c447ffb155c8",
            "7501c2e30fb74ffc8c2d0a688737d544",
            "749edb6859ed40fea66c53f567a80c45",
            "c3d32c24053e46f7b5d360e4c6a2ba3b",
            "9d0b36769b15439bbfdb4e325ca105c0"
          ]
        },
        "id": "oIudp03hAWRx",
        "outputId": "a1e5ac96-51a2-4303-f3a2-7652170595fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/32.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bff2d2cf3ea47339287e5509d320526"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-3358945452>:202: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-5-3358945452>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 1: 100%|██████████| 183/183 [03:41<00:00,  1.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 1.1593, Acc: 0.5446\n",
            "  Class counts: [585, 549, 605, 580, 599]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.6712, Acc: 0.7425\n",
            "  P: 0.6236, R: 0.6438, F1: 0.6097\n",
            "  CM:\n",
            " [[170   5   4   0   0]\n",
            " [  8  19   7   1   2]\n",
            " [  6   1  51  19  23]\n",
            " [  0   0   4  11   4]\n",
            " [  0   0   4   6  20]]\n",
            ">> Best DenseNet-121 model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-5-3358945452>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [03:32<00:00,  1.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.6792, Acc: 0.7416\n",
            "  Class counts: [572, 561, 579, 617, 589]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 0.6643, Acc: 0.7342\n",
            "  P: 0.5921, R: 0.6196, F1: 0.5799\n",
            "  CM:\n",
            " [[172   6   1   0   0]\n",
            " [  8  24   4   0   1]\n",
            " [  3   6  49  27  15]\n",
            " [  0   0   1  12   6]\n",
            " [  0   1   6  12  11]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-5-3358945452>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:33<00:00,  1.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 0.6160, Acc: 0.7683\n",
            "  Class counts: [582, 604, 592, 562, 578]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 0.4801, Acc: 0.8164\n",
            "  P: 0.7449, R: 0.6806, F1: 0.6839\n",
            "  CM:\n",
            " [[172   7   0   0   0]\n",
            " [  8  27   2   0   0]\n",
            " [  5   9  73   1  12]\n",
            " [  1   1   5   6   6]\n",
            " [  0   2   8   0  20]]\n",
            ">> Best DenseNet-121 model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-5-3358945452>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:37<00:00,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 0.5911, Acc: 0.7766\n",
            "  Class counts: [558, 597, 579, 550, 634]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 0.7943, Acc: 0.7315\n",
            "  P: 0.5900, R: 0.6415, F1: 0.5771\n",
            "  CM:\n",
            " [[171   8   0   0   0]\n",
            " [  5  31   1   0   0]\n",
            " [  4  15  43  28  10]\n",
            " [  0   3   2  13   1]\n",
            " [  0   3  10   8   9]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-5-3358945452>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:39<00:00,  1.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 0.5322, Acc: 0.7999\n",
            "  Class counts: [607, 562, 567, 603, 579]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 0.6664, Acc: 0.7452\n",
            "  P: 0.6263, R: 0.6305, F1: 0.6000\n",
            "  CM:\n",
            " [[173   5   1   0   0]\n",
            " [  9  21   5   1   1]\n",
            " [  3   2  52  31  12]\n",
            " [  0   0   1  12   6]\n",
            " [  0   0   6  10  14]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-5-3358945452>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:38<00:00,  1.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 0.4797, Acc: 0.8136\n",
            "  Class counts: [591, 588, 563, 609, 567]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 0.7171, Acc: 0.7890\n",
            "  P: 0.6350, R: 0.6588, F1: 0.6440\n",
            "  CM:\n",
            " [[168   7   2   1   1]\n",
            " [  6  25   6   0   0]\n",
            " [  0   4  72  12  12]\n",
            " [  0   2   4  10   3]\n",
            " [  0   1  12   4  13]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 7:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-5-3358945452>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 7: 100%|██████████| 183/183 [03:34<00:00,  1.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Train Loss: 0.4046, Acc: 0.8454\n",
            "  Class counts: [547, 630, 549, 634, 558]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Valid Loss: 0.5939, Acc: 0.7973\n",
            "  P: 0.6776, R: 0.6785, F1: 0.6659\n",
            "  CM:\n",
            " [[175   4   0   0   0]\n",
            " [  7  29   1   0   0]\n",
            " [  4  14  61   3  18]\n",
            " [  0   2   5   8   4]\n",
            " [  0   2   8   2  18]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 8:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-5-3358945452>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 8: 100%|██████████| 183/183 [03:42<00:00,  1.22s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 008 — Train Loss: 0.3599, Acc: 0.8646\n",
            "  Class counts: [572, 594, 563, 600, 589]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 008 — Valid Loss: 0.8355, Acc: 0.7616\n",
            "  P: 0.5842, R: 0.5842, F1: 0.5774\n",
            "  CM:\n",
            " [[171   7   0   0   1]\n",
            " [  5  28   4   0   0]\n",
            " [  2  14  66   5  13]\n",
            " [  0   0   9   6   4]\n",
            " [  0   3  18   2   7]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [00:24<00:00,  1.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance (DenseNet-121) ===\n",
            "Test Loss : 0.4896\n",
            "Test Acc  : 0.8110\n",
            "Precision : 0.6522\n",
            "Recall    : 0.6497\n",
            "F1-score  : 0.6264\n",
            "Confusion Matrix:\n",
            "[[178   2   0   0   0]\n",
            " [  3  31   3   0   0]\n",
            " [  4  11  66   1  17]\n",
            " [  1   0   9   2   8]\n",
            " [  0   0   8   2  19]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''ViT (Vision Transformer)\n",
        "seed 고정 APTOS 데이터만 사용\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만 거치게 됩니다.\n",
        "WeightedRandomsampler 사용\n",
        "preprocess_size를 (224, 224)\n",
        "나머지 학습 루프(optimizer, scheduler, loss)는 기본 shuffle + nn.CrossEntropyLoss() 세팅 그대로\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# -------------------------------\n",
        "# 실험 재현성 위한 Seed 고정 함수 추가\n",
        "# -------------------------------\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)  # 코드 제일 앞에서 실행!\n",
        "\n",
        "# -------------------------------\n",
        "# 이하 기존 코드 (ViT 적용을 위해 변경된 부분만 주석 처리)\n",
        "# -------------------------------\n",
        "\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(224, 224)):  # ViT 기본 입력 크기\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (On-DAT 클래스별 증강)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(224,224)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "\n",
        "        # class2, 3, 4만 증강, 나머지는 base transform만 적용\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 파일 존재 필터링\n",
        "# ----------------------------------------\n",
        "ROOT     = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path = os.path.join(ROOT, 'train.csv')\n",
        "img_dir  = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image']\n",
        "    .apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10) - random_state 고정!\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = 1.0 / class_counts\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler  = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (Sampler 적용)\n",
        "# ----------------------------------------\n",
        "train_ds     = APTOSDataset(df_train, img_dir)\n",
        "val_ds       = APTOSDataset(df_val,   img_dir)\n",
        "test_ds      = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'vit_base_patch16_224',  # ViT Base patch16 224×224\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", counts)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc; no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_vit_base_patch16_224_aptos.pth')\n",
        "        print(\">> Best ViT-Base model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'vit_base_patch16_224',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_vit_base_patch16_224_aptos.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance (ViT Base Patch16 224) ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d501f384672946d4947dab7598f54d7e",
            "5744087039d7475fb832e88ce51f1ee1",
            "02b414a831b8441c945b90eae39b770b",
            "06887e4efc834e1b885cd4b5fc866c08",
            "81bc53dc5c524a089db6e4517d452dc8",
            "fc574cce2f4e4fe4bf02cf6a493bee42",
            "73127dd16b874d27b3ba3f3568297875",
            "b02d4dc537bf4f4b8608921b2f40aca6",
            "7c44d5d07e0f43e993f98a96dd3c456b",
            "d91528097d3848c1bdb5fa75b080977c",
            "31597833e6ad4f4298e1220fa1a70224"
          ]
        },
        "id": "MsOOHYJ4A3fF",
        "outputId": "d97c913a-54af-4d98-dccb-c65cb38cd2a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d501f384672946d4947dab7598f54d7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-2212772727>:202: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2212772727>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "Train Epoch 1: 100%|██████████| 183/183 [03:20<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 1.0339, Acc: 0.5562\n",
            "  Class counts: [604, 575, 585, 573, 581]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.8228, Acc: 0.6493\n",
            "  P: 0.5828, R: 0.5708, F1: 0.4924\n",
            "  CM:\n",
            " [[149  23   5   2   0]\n",
            " [  4  19  14   0   0]\n",
            " [  2   1  48  48   1]\n",
            " [  0   0   1  17   1]\n",
            " [  0   0   6  20   4]]\n",
            ">> Best ViT-Base model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2212772727>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [03:24<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.9054, Acc: 0.6234\n",
            "  Class counts: [613, 582, 570, 592, 561]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 1.4204, Acc: 0.4740\n",
            "  P: 0.3845, R: 0.4105, F1: 0.3183\n",
            "  CM:\n",
            " [[129   3   6   0  41]\n",
            " [  5  14   5   0  13]\n",
            " [  3   3   2   0  92]\n",
            " [  0   0   0   0  19]\n",
            " [  1   0   1   0  28]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2212772727>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:18<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 1.1823, Acc: 0.5041\n",
            "  Class counts: [574, 595, 580, 611, 558]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 1.0656, Acc: 0.5671\n",
            "  P: 0.2790, R: 0.4976, F1: 0.3302\n",
            "  CM:\n",
            " [[164  12   0   3   0]\n",
            " [  8  27   0   2   0]\n",
            " [ 18  18   0  64   0]\n",
            " [  0   3   0  16   0]\n",
            " [  2  10   0  18   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2212772727>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:21<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 1.2401, Acc: 0.4777\n",
            "  Class counts: [624, 548, 593, 562, 591]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 1.3333, Acc: 0.6164\n",
            "  P: 0.4076, R: 0.3807, F1: 0.3561\n",
            "  CM:\n",
            " [[121   2  56   0   0]\n",
            " [  5  11  21   0   0]\n",
            " [  4   3  93   0   0]\n",
            " [  0   0  19   0   0]\n",
            " [  1   0  29   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2212772727>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 1.4190, Acc: 0.3811\n",
            "  Class counts: [575, 593, 579, 595, 576]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 1.5456, Acc: 0.2603\n",
            "  P: 0.1399, R: 0.2191, F1: 0.1270\n",
            "  CM:\n",
            " [[83  0  1 95  0]\n",
            " [ 5  0  0 32  0]\n",
            " [26  0  0 74  0]\n",
            " [ 7  0  0 12  0]\n",
            " [ 7  0  0 23  0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-6-2212772727>:231: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:21<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 1.5742, Acc: 0.2711\n",
            "  Class counts: [555, 543, 616, 609, 595]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 1.8096, Acc: 0.1233\n",
            "  P: 0.2664, R: 0.2211, F1: 0.0830\n",
            "  CM:\n",
            " [[  0 159  20   0   0]\n",
            " [  0  36   1   0   0]\n",
            " [  0  92   8   0   0]\n",
            " [  0  13   5   1   0]\n",
            " [  0  28   2   0   0]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [00:24<00:00,  1.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance (ViT Base Patch16 224) ===\n",
            "Test Loss : 0.7469\n",
            "Test Acc  : 0.6685\n",
            "Precision : 0.5998\n",
            "Recall    : 0.5926\n",
            "F1-score  : 0.5254\n",
            "Confusion Matrix:\n",
            "[[147  24   9   0   0]\n",
            " [  1  24  10   2   0]\n",
            " [  2   2  52  43   0]\n",
            " [  0   1   1  16   2]\n",
            " [  0   0  13  11   5]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''ConvNeXt-Base\n",
        "seed 고정 APTOS 데이터만 사용\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만 거치게 됩니다.\n",
        "WeightedRandomsampler 사용\n",
        "preprocess_size를 (224, 224)\n",
        "나머지 학습 루프(optimizer, scheduler, loss)는 기본 shuffle + nn.CrossEntropyLoss() 세팅 그대로\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# -------------------------------\n",
        "# 실험 재현성 위한 Seed 고정 함수 추가\n",
        "# -------------------------------\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)  # 코드 제일 앞에서 실행!\n",
        "\n",
        "# -------------------------------\n",
        "# 필요한 라이브러리 임포트\n",
        "# -------------------------------\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(224, 224)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (On-DAT 클래스별 증강)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(224,224)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "\n",
        "        # class2, 3, 4만 증강, 나머지는 base transform만 적용\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 파일 존재 필터링\n",
        "# ----------------------------------------\n",
        "ROOT     = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path = os.path.join(ROOT, 'train.csv')\n",
        "img_dir  = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image']\n",
        "    .apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10) - random_state 고정!\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = 1.0 / class_counts\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler  = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (Sampler 적용)\n",
        "# ----------------------------------------\n",
        "train_ds     = APTOSDataset(df_train, img_dir)\n",
        "val_ds       = APTOSDataset(df_val,   img_dir)\n",
        "test_ds      = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=train_sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'convnext_base',   # ConvNeXt-Base 사용\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", counts)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc; no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_convnext_base_aptos.pth')\n",
        "        print(\">> Best ConvNeXt-Base model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'convnext_base',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_convnext_base_aptos.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance (ConvNeXt-Base) ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "861193351aa64edcb8ebedf1e6a8b954",
            "7f3e9707a280482dbb1038f4d8cce71f",
            "fa8a138a8f9942138a2c33166dba694d",
            "cd9d90cf066e449dbfe0667e6af1cc38",
            "99af4260364848718a889c42b0b10730",
            "35a1d6bb63b744388dd6a203b81852ec",
            "83191c0995294e3fbaa6939b2936900d",
            "71b547f5bccb4adf83ca2bd4a4cb5bb9",
            "8af51e3d2c304c578332bbd3de6963b2",
            "cba1d54aac3944b0a393ea3d82bc3df5",
            "6bf04142c91d4966b120059a115b57ab"
          ]
        },
        "id": "Vvr5OppRBa6i",
        "outputId": "6b2866eb-0bf1-4ff3-e1ec-a257c70907fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "861193351aa64edcb8ebedf1e6a8b954"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-1795467977>:201: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-7-1795467977>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 1: 100%|██████████| 183/183 [03:25<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 0.8184, Acc: 0.6576\n",
            "  Class counts: [603, 608, 524, 580, 603]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.5370, Acc: 0.8000\n",
            "  P: 0.6978, R: 0.6990, F1: 0.6637\n",
            "  CM:\n",
            " [[173   6   0   0   0]\n",
            " [  2  31   4   0   0]\n",
            " [  3  10  64  21   2]\n",
            " [  0   0   5  13   1]\n",
            " [  0   1   8  10  11]]\n",
            ">> Best ConvNeXt-Base model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-7-1795467977>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [03:24<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.5667, Acc: 0.7862\n",
            "  Class counts: [576, 638, 587, 566, 551]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 0.4771, Acc: 0.8274\n",
            "  P: 0.7947, R: 0.5979, F1: 0.6401\n",
            "  CM:\n",
            " [[179   0   0   0   0]\n",
            " [ 10  12  15   0   0]\n",
            " [  2   0  93   2   3]\n",
            " [  0   0   9   7   3]\n",
            " [  1   0  17   1  11]]\n",
            ">> Best ConvNeXt-Base model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-7-1795467977>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [03:24<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 0.5107, Acc: 0.8060\n",
            "  Class counts: [568, 618, 615, 560, 557]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 0.6212, Acc: 0.8055\n",
            "  P: 0.7065, R: 0.6478, F1: 0.6693\n",
            "  CM:\n",
            " [[178   1   0   0   0]\n",
            " [ 15  19   3   0   0]\n",
            " [  6   8  71   4  11]\n",
            " [  0   0   8   8   3]\n",
            " [  1   2   9   0  18]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-7-1795467977>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:25<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 0.5284, Acc: 0.8029\n",
            "  Class counts: [579, 572, 594, 584, 589]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 0.7494, Acc: 0.7479\n",
            "  P: 0.6349, R: 0.6424, F1: 0.5893\n",
            "  CM:\n",
            " [[172   7   0   0   0]\n",
            " [  7  30   0   0   0]\n",
            " [  3  20  43   3  31]\n",
            " [  0   1   4   4  10]\n",
            " [  0   2   3   1  24]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-7-1795467977>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:22<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 0.4999, Acc: 0.8033\n",
            "  Class counts: [601, 580, 615, 551, 571]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 0.6491, Acc: 0.7726\n",
            "  P: 0.6048, R: 0.6477, F1: 0.6194\n",
            "  CM:\n",
            " [[165  10   0   0   4]\n",
            " [  1  32   4   0   0]\n",
            " [  1  12  65  10  12]\n",
            " [  0   0   7   7   5]\n",
            " [  0   1  12   4  13]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-7-1795467977>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:23<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 0.4467, Acc: 0.8338\n",
            "  Class counts: [607, 577, 577, 586, 571]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 0.6099, Acc: 0.8219\n",
            "  P: 0.7232, R: 0.6854, F1: 0.6771\n",
            "  CM:\n",
            " [[171   7   1   0   0]\n",
            " [  7  24   5   0   1]\n",
            " [  3   7  76   1  13]\n",
            " [  0   0   6   5   8]\n",
            " [  0   2   3   1  24]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 7:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-7-1795467977>:230: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 7: 100%|██████████| 183/183 [03:21<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Train Loss: 0.3658, Acc: 0.8653\n",
            "  Class counts: [575, 601, 576, 588, 578]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Valid Loss: 0.5312, Acc: 0.8137\n",
            "  P: 0.6994, R: 0.7261, F1: 0.6945\n",
            "  CM:\n",
            " [[172   7   0   0   0]\n",
            " [  2  33   2   0   0]\n",
            " [  1  20  66   7   6]\n",
            " [  0   1   4  13   1]\n",
            " [  0   4   9   4  13]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [00:24<00:00,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance (ConvNeXt-Base) ===\n",
            "Test Loss : 0.4714\n",
            "Test Acc  : 0.8329\n",
            "Precision : 0.7533\n",
            "Recall    : 0.5894\n",
            "F1-score  : 0.6118\n",
            "Confusion Matrix:\n",
            "[[180   0   0   0   0]\n",
            " [  6  10  21   0   0]\n",
            " [  0   0  95   2   2]\n",
            " [  0   0   8   4   8]\n",
            " [  0   0  12   2  15]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/best_efficientnetb3_aptos.pth /content/drive/MyDrive/DL_Project_17/YJ/APTOS/best_efficientnetb3_aptos.pth\n",
        "!mv /content/best_convnext_base_aptos.pth /content/drive/MyDrive/DL_Project_17/YJ/APTOS/best_convnext_base_aptos.pth\n",
        "!mv /content/best_vit_base_patch16_224_aptos.pth /content/drive/MyDrive/DL_Project_17/YJ/APTOS/best_vit_base_patch16_224_aptos.pth\n",
        "!mv /content/best_densenet121_aptos.pth /content/drive/MyDrive/DL_Project_17/YJ/APTOS/best_densenet121_aptos.pth"
      ],
      "metadata": {
        "id": "d2KD5dNDBo1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''(추가 !!!!!!!) 잘못된 방법 APTOS 데이터만 사용, StratifiedShuffleSplit을 두 단계로 사용해 전체 APTOS를 80/10/10으로 나눴고,\n",
        "class2·3·4만 On-DAT 증강을 매 배치마다 자동으로 받고, class0·1은 원래 전처리만 거치게 됩니다.\n",
        "WeightedRandomsampler를 쓰니 학습은 균일 분포로 하는데 test는 불균형 분포를 그대로 해서 train은 안나오고 test는 잘나오는 현상이 생김\n",
        "그래서 WeightRamdomsampler는 제거, Sampler 대신 Loss 함수에 클래스별 가중치를 주는 방법으로 전환 - CrossEntropyLoss(weight=weights_tensor) 적용\n",
        "나머지 학습 루프(optimizer, scheduler, loss)는 기본 shuffle + nn.CrossEntropyLoss() 세팅 그대로\n",
        "APTOSDataset 클래스에서 .png 확장자를 붙여 이미지를 로드합니다.\n",
        "OneCycleLR + AMP 학습 루프, 검증, 최종 테스트까지 포함'''\n",
        "\n",
        "# Colab에서 실행 시 필요한 라이브러리 설치\n",
        "# !pip install opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (On-DAT 클래스별 증강)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384)):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "\n",
        "        self.base_transform = A.Compose([\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Rotate(limit=15, p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img = np.array(pil)\n",
        "\n",
        "        if label >= 2:\n",
        "            img_t = self.aug_transform(image=img)['image']\n",
        "        else:\n",
        "            img_t = self.base_transform(image=img)['image']\n",
        "\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 파일 존재 필터링\n",
        "# ----------------------------------------\n",
        "ROOT    = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path= os.path.join(ROOT, 'train.csv')\n",
        "img_dir = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[full_df['image']\n",
        "    .apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png')))]\n",
        "full_df = full_df.reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 중, 실제 파일이 있는 {len(full_df)}개만 사용합니다.\")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10)\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, hold_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_hold  = full_df.iloc[hold_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_hold, df_hold['label']))\n",
        "df_val  = df_hold.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_hold.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) 클래스 가중치 기반 Loss 세팅\n",
        "# ----------------------------------------\n",
        "class_counts   = df_train['label'].value_counts().sort_index().values\n",
        "class_weights  = 1.0 / class_counts\n",
        "weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (불균형 shuffle)\n",
        "# ----------------------------------------\n",
        "train_ds = APTOSDataset(df_train, img_dir)\n",
        "val_ds   = APTOSDataset(df_val,   img_dir)\n",
        "test_ds  = APTOSDataset(df_test,  img_dir)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    counts = [0]*num_classes\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist():\n",
        "            counts[l] += 1\n",
        "\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"\\nEpoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "    print(\"  Class counts:\", counts)\n",
        "\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  CM:\\n\", v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc; no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos_weighted.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\"); break\n",
        "\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos_weighted.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj1eARNOX4kd",
        "outputId": "0abb6dd1-42be-4395-f39b-5ba29f19776a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 3648개 중, 실제 파일이 있는 3648개만 사용합니다.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-2117367826>:184: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-9-2117367826>:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "Train Epoch 1: 100%|██████████| 183/183 [02:47<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Train Loss: 0.9807, Acc: 0.7032\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 001 — Valid Loss: 0.8099, Acc: 0.6548\n",
            "  P: 0.5609, R: 0.6098, F1: 0.5028\n",
            "  CM:\n",
            " [[163  15   1   0   0]\n",
            " [  3  31   1   2   0]\n",
            " [  0  12  16  25  47]\n",
            " [  0   1   0   9   9]\n",
            " [  0   2   1   7  20]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-9-2117367826>:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [02:50<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Train Loss: 0.8796, Acc: 0.7502\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 002 — Valid Loss: 0.9974, Acc: 0.6575\n",
            "  P: 0.5191, R: 0.5114, F1: 0.4210\n",
            "  CM:\n",
            " [[179   0   0   0   0]\n",
            " [ 16  20   1   0   0]\n",
            " [  4  13  15   0  68]\n",
            " [  0   1   0   0  18]\n",
            " [  0   4   0   0  26]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-9-2117367826>:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [02:50<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Train Loss: 1.0702, Acc: 0.6439\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 003 — Valid Loss: 1.2353, Acc: 0.5753\n",
            "  P: 0.4080, R: 0.4365, F1: 0.3778\n",
            "  CM:\n",
            " [[149  27   0   0   3]\n",
            " [  5  30   0   0   2]\n",
            " [ 16  29  22   5  28]\n",
            " [  0   5   6   1   7]\n",
            " [  3   7   9   3   8]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-9-2117367826>:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [02:50<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Train Loss: 1.2913, Acc: 0.5651\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 004 — Valid Loss: 1.4644, Acc: 0.6164\n",
            "  P: 0.3418, R: 0.3947, F1: 0.3355\n",
            "  CM:\n",
            " [[163  16   0   0   0]\n",
            " [ 10  26   1   0   0]\n",
            " [ 11  53  36   0   0]\n",
            " [  1   8  10   0   0]\n",
            " [  2  17  11   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-9-2117367826>:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [02:47<00:00,  1.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Train Loss: 1.5880, Acc: 0.2947\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 005 — Valid Loss: 1.6166, Acc: 0.2740\n",
            "  P: 0.0548, R: 0.2000, F1: 0.0860\n",
            "  CM:\n",
            " [[  0   0 179   0   0]\n",
            " [  0   0  37   0   0]\n",
            " [  0   0 100   0   0]\n",
            " [  0   0  19   0   0]\n",
            " [  0   0  30   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-9-2117367826>:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [02:49<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Train Loss: 1.6230, Acc: 0.2526\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 006 — Valid Loss: 1.6184, Acc: 0.1014\n",
            "  P: 0.0203, R: 0.2000, F1: 0.0368\n",
            "  CM:\n",
            " [[  0 179   0   0   0]\n",
            " [  0  37   0   0   0]\n",
            " [  0 100   0   0   0]\n",
            " [  0  19   0   0   0]\n",
            " [  0  30   0   0   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 7:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-9-2117367826>:214: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 7: 100%|██████████| 183/183 [02:49<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Train Loss: 1.6183, Acc: 0.3561\n",
            "  Class counts: [1437, 295, 796, 154, 236]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:24<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 007 — Valid Loss: 1.6119, Acc: 0.2740\n",
            "  P: 0.0548, R: 0.2000, F1: 0.0860\n",
            "  CM:\n",
            " [[  0   0 179   0   0]\n",
            " [  0   0  37   0   0]\n",
            " [  0   0 100   0   0]\n",
            " [  0   0  19   0   0]\n",
            " [  0   0  30   0   0]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [00:26<00:00,  1.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance ===\n",
            "Test Loss : 0.9346\n",
            "Test Acc  : 0.6904\n",
            "Precision : 0.5754\n",
            "Recall    : 0.5709\n",
            "F1-score  : 0.4741\n",
            "Confusion Matrix:\n",
            "[[179   1   0   0   0]\n",
            " [ 11  26   0   0   0]\n",
            " [  1   8  19   0  71]\n",
            " [  0   1   0   0  19]\n",
            " [  0   1   0   0  28]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!mv /content/best_swin_large384_aptos_aug_2.pth /content/drive/MyDrive/DL_Project_17/YJ/APTOS/best_swin_large384_aptos_aug_2.pth\n",
        "!mv /content/best_swin_large384_aptos_aug_3.pth /content/drive/MyDrive/DL_Project_17/YJ/APTOS/best_swin_large384_aptos_aug_3.pth"
      ],
      "metadata": {
        "id": "tG0Gq9qUkyCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d-8zL_6uvMac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''APTOS 전체 데이터, OnDAT 1차, WeightedRamdomSampler 적용\n",
        "RandomRotate\n",
        "RandomBrightnessContrast\n",
        "GaussianNoise\n",
        "HueSaturationValue\n",
        "Cutout (RandomErasing)\n",
        "(참고: Albumentations 라이브러리 기반, 각 증강 강도는 안저(fundus) 영상 문헌에서 자주 쓰이는 범위로 설정)'''\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a_chan, b_chan = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a_chan, b_chan])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (증강 포함)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384), train=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "        self.train = train\n",
        "\n",
        "        if self.train:\n",
        "            self.aug = A.Compose([\n",
        "                A.RandomRotate90(p=0.5),\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n",
        "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.5),\n",
        "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
        "                A.CoarseDropout(              # 수정: Cutout -> CoarseDropout\n",
        "                    min_holes=1,\n",
        "                    max_holes=8,\n",
        "                    max_height=32,\n",
        "                    max_width=32,\n",
        "                    fill_value=0,\n",
        "                    p=0.5\n",
        "                ),\n",
        "                A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "        else:\n",
        "            self.aug = A.Compose([\n",
        "                A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil_img = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img_np  = np.array(pil_img)\n",
        "        img_t   = self.aug(image=img_np)['image']\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 필터링\n",
        "# ----------------------------------------\n",
        "ROOT     = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path = os.path.join(ROOT, 'train.csv')\n",
        "img_dir  = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "\n",
        "def file_exists(fname):\n",
        "    if not fname.lower().endswith('.png'):\n",
        "        fname = f\"{fname}.png\"\n",
        "    return os.path.isfile(os.path.join(img_dir, fname))\n",
        "\n",
        "full_df = full_df[ full_df['image'].apply(file_exists) ].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10)\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, holdout_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train   = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_holdout = full_df.iloc[holdout_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_holdout, df_holdout['label']))\n",
        "df_val  = df_holdout.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_holdout.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 준비\n",
        "# ----------------------------------------\n",
        "class_counts = df_train['label'].value_counts().sort_index().values\n",
        "class_weights = 1.0 / class_counts\n",
        "sample_weights = class_weights[df_train['label'].values]\n",
        "sampler = WeightedRandomSampler(weights=sample_weights,\n",
        "                                num_samples=len(sample_weights),\n",
        "                                replacement=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비 (sampler 사용)\n",
        "# ----------------------------------------\n",
        "train_ds = APTOSDataset(df_train, img_dir, train=True)\n",
        "val_ds   = APTOSDataset(df_val,   img_dir, train=False)\n",
        "test_ds  = APTOSDataset(df_test,  img_dir, train=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True,\n",
        "    num_classes=5\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1,\n",
        "    div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    # — Train —\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss/t_total\n",
        "    train_acc  = t_correct/t_total\n",
        "    print(f\"Epoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "\n",
        "    # — Validate —\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu())\n",
        "            all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss/v_total\n",
        "    valid_acc  = v_correct/v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"Epoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  Confusion Matrix:\")\n",
        "    print(v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc\n",
        "        no_improve   = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=False, num_classes=5\n",
        ").to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu())\n",
        "        all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss/t_total\n",
        "test_acc  = t_correct/t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dbf1d3a716fe441794d68fa708957f4c",
            "df59f27d0e8749c8880cace12b4ef95b",
            "eebab466632b4e4bbce57a534753635c",
            "0eecafd8acfe480782b7f19c76f606bc",
            "025b95c8495b41b08b20efd3101a96d6",
            "c98a9526f0c44ea29549f6890fe6ff7a",
            "f708de65fc184f6c8983dcb1b322ed8e",
            "d6fb6b2b31b24254b205039aa2294a8a",
            "2c8e935222cb4d94892a6ba71ed5856b",
            "f0d2cbd49458484fafc1806146e5bbfe",
            "e2b59e7cc1264ce8833992749ee60425"
          ]
        },
        "id": "dpFcTOKrWKfU",
        "outputId": "6e7ce485-4959-40b4-b0a3-100aa8c6d7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n",
            "<ipython-input-4-3793b9488e2a>:86: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
            "<ipython-input-4-3793b9488e2a>:87: UserWarning: Argument(s) 'min_holes, max_holes, max_height, max_width, fill_value' are not valid for transform CoarseDropout\n",
            "  A.CoarseDropout(              # 수정: Cutout -> CoarseDropout\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/801M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbf1d3a716fe441794d68fa708957f4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-3793b9488e2a>:188: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler     = GradScaler()\n",
            "Train Epoch 1:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3793b9488e2a>:213: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 1: 100%|██████████| 183/183 [06:13<00:00,  2.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 — Train Loss: 1.2116, Acc: 0.4517\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [01:18<00:00,  3.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 — Valid Loss: 0.7155, Acc: 0.7370\n",
            "  P: 0.6144, R: 0.6867, F1: 0.6151\n",
            "  Confusion Matrix:\n",
            "[[167  11   1   0   0]\n",
            " [  1  27   9   0   0]\n",
            " [  1  14  42  30  13]\n",
            " [  0   0   3  13   3]\n",
            " [  0   0   0  10  20]]\n",
            ">> Best model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 2:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3793b9488e2a>:213: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 2: 100%|██████████| 183/183 [04:40<00:00,  1.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 002 — Train Loss: 1.0654, Acc: 0.5480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 002 — Valid Loss: 0.7595, Acc: 0.7288\n",
            "  P: 0.6562, R: 0.6054, F1: 0.5797\n",
            "  Confusion Matrix:\n",
            "[[171   7   1   0   0]\n",
            " [  4  31   2   0   0]\n",
            " [  1  45  46   8   0]\n",
            " [  0   1   7   9   2]\n",
            " [  0   9   9   3   9]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 3:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3793b9488e2a>:213: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 3: 100%|██████████| 183/183 [04:12<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 003 — Train Loss: 1.1855, Acc: 0.5048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 003 — Valid Loss: 1.1260, Acc: 0.6137\n",
            "  P: 0.4776, R: 0.5107, F1: 0.4554\n",
            "  Confusion Matrix:\n",
            "[[138  25   4   0  12]\n",
            " [  2  27   3   1   4]\n",
            " [  1   8  40   8  43]\n",
            " [  0   0   2   1  16]\n",
            " [  0   1   9   2  18]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 4:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3793b9488e2a>:213: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 4: 100%|██████████| 183/183 [03:55<00:00,  1.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 004 — Train Loss: 1.2658, Acc: 0.4383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:22<00:00,  1.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 004 — Valid Loss: 0.9424, Acc: 0.5836\n",
            "  P: 0.3625, R: 0.5326, F1: 0.3478\n",
            "  Confusion Matrix:\n",
            "[[165  10   2   2   0]\n",
            " [  6  29   0   2   0]\n",
            " [  5  32   1  62   0]\n",
            " [  0   1   0  18   0]\n",
            " [  0   5   0  25   0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 5:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3793b9488e2a>:213: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 5: 100%|██████████| 183/183 [03:56<00:00,  1.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 005 — Train Loss: 1.2610, Acc: 0.4489\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 005 — Valid Loss: 0.8103, Acc: 0.6740\n",
            "  P: 0.5177, R: 0.5326, F1: 0.4896\n",
            "  Confusion Matrix:\n",
            "[[171   7   1   0   0]\n",
            " [  7  23   6   1   0]\n",
            " [  8  25  36  27   4]\n",
            " [  0   0   6  10   3]\n",
            " [  1  10   9   4   6]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTrain Epoch 6:   0%|          | 0/183 [00:00<?, ?it/s]<ipython-input-4-3793b9488e2a>:213: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Train Epoch 6: 100%|██████████| 183/183 [03:44<00:00,  1.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 006 — Train Loss: 1.1577, Acc: 0.4774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid: 100%|██████████| 23/23 [00:23<00:00,  1.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 006 — Valid Loss: 0.7933, Acc: 0.6932\n",
            "  P: 0.4496, R: 0.5203, F1: 0.4596\n",
            "  Confusion Matrix:\n",
            "[[173   6   0   0   0]\n",
            " [ 10  14  12   1   0]\n",
            " [ 15   5  52  28   0]\n",
            " [  1   0   4  14   0]\n",
            " [  1   3  16  10   0]]\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Test: 100%|██████████| 23/23 [01:24<00:00,  3.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Test Set Performance ===\n",
            "Test Loss : 0.6614\n",
            "Test Acc  : 0.7671\n",
            "Precision : 0.6127\n",
            "Recall    : 0.6571\n",
            "F1-score  : 0.6178\n",
            "Confusion Matrix:\n",
            "[[173   7   0   0   0]\n",
            " [  0  22   9   3   3]\n",
            " [  0   9  55  22  13]\n",
            " [  0   0   4   9   7]\n",
            " [  0   1   1   6  21]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. 증강 강도 먼저 줄여보기\n",
        "  - OnDAT의 블러 반경, 밝기·대비·색조 변화 범위 등을 절반 정도로 낮추고, 전체 적용 확률 p를 1.0 → 0.5로 줄여보세요.\n",
        "    이렇게 하면 모델이 여전히 원본 특징을 어느 정도 학습하면서도 약간의 변형에 대응하도록 훈련됩니다.\n",
        "2. Sampler 세팅 약간 튜닝하기\n",
        "  - replacement=True일 때 소수 클래스 샘플이 반복해서 뽑히는 게 부담스럽다면\n",
        "    num_samples=len(train_ds) 대신 num_samples = 0.5 * len(train_ds) 등으로 줄여 본다거나\n",
        "    replacement=False로 바꿔서 “각 이미지는 최대 1번” 뽑히게 해보세요.\n",
        "    이렇게 하면 소수 클래스가 과도하게 중복되지 않습니다.\n",
        "3. 클래스별 증강 비율 다르게 주기\n",
        "  - 소수 클래스(예: 클래스 1, 3)에는 강한 증강을 70% 확률로, 다수 클래스(예: 클래스 0, 2)에는 30% 확률로만 적용하도록 분기 처리해 보세요.\n",
        "    클래스별 sampler와 증강을 결합하면, 양쪽 효과를 더 세밀하게 조절할 수 있습니다.\n",
        "4. 스케줄 증강\n",
        "  - 1–5 epoch: 증강 거의 없이 학습\n",
        "  - 6–15 epoch: 가벼운 증강\n",
        "  - 16–최종: 강한 증강\n",
        "이렇게 나눠서, 처음에는 안정적으로 수렴시키고 점차 일반화 능력을 끌어올립니다.\n",
        "5. 지표 추적 & 비교\n",
        "  - 매 실험마다 train/val loss, accuracy, macro-F1, 클래스별 recall 변화를 표로 정리하세요.\n",
        "    어떤 조합이 가장 밸런스 있게 성능을 뽑아내는지 한눈에 비교하기 좋습니다.\n",
        "\n",
        "\n",
        "1. 베이스라인 확인\n",
        "Augmentation: OFF\n",
        "Sampler: OFF\n",
        "목표: 기존 성능(Acc≈0.77, Val Acc≈0.82) 확인\n",
        "\n",
        "2. 약한 증강 적용\n",
        "Augmentation: ON (강도 ↓, 적용 확률 p=0.5)\n",
        "블러·밝기·대비 등 파라미터 절반\n",
        "Sampler: OFF\n",
        "목표: 약한 증강만으로 일반화 개선 여부\n",
        "\n",
        "3. WeightedSampler 단독 적용\n",
        "Augmentation: OFF\n",
        "Sampler: ON (replacement=True, num_samples=len(train))\n",
        "목표: 소수 클래스 오버샘플링 효과 확인\n",
        "\n",
        "4. 증강 + Sampler (기본)\n",
        "Augmentation: ON (기본 OnDAT 세팅)\n",
        "Sampler: ON (replacement=True)\n",
        "목표: 두 기법 결합 시 시너지/부작용\n",
        "\n",
        "5. Sampler 튜닝\n",
        "Augmentation: OFF\n",
        "Sampler: ON\n",
        "replacement=False\n",
        "또는 num_samples = 0.5 * len(train)\n",
        "목표: 중복 샘플링 줄인 버전과 비교\n",
        "\n",
        "6. 약한 증강 + Sampler 튜닝\n",
        "Augmentation: ON (강도 ↓, p=0.5)\n",
        "Sampler: ON (replacement=False or num_samples=0.5*len)\n",
        "목표: 과도한 증강/샘플링 모두 완화\n",
        "\n",
        "7. 클래스별 맞춤 증강\n",
        "Augmentation: ON\n",
        "소수 클래스 p=0.7, 다수 클래스 p=0.3\n",
        "Sampler: ON (replacement=False)\n",
        "목표: 클래스별 증강 차별화 효과\n",
        "\n",
        "8. 스케줄형 증강\n",
        "Epoch 1–5: Aug OFF\n",
        "Epoch 6–15: Aug ON (약하게)\n",
        "Epoch 16+: Aug ON (강하게)\n",
        "Sampler: ON (replacement=False)\n",
        "목표: 단계별 적응학습 효과'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "kLR8SGo2fq0r",
        "outputId": "45d2b55f-8220-40f4-bf52-845dc0f9794c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'APTOS 전체 데이터, OnDAT, WeightedRamdomSampler 적용\\nRandomRotate\\nRandomBrightnessContrast\\nGaussianNoise\\nHueSaturationValue\\nCutout (RandomErasing)\\n(참고: Albumentations 라이브러리 기반, 각 증강 강도는 안저(fundus) 영상 문헌에서 자주 쓰이는 범위로 설정)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''sampler off, OnDAT 약하게 적용'''\n",
        "\n",
        "# Colab에서 실행 시 필요한 라이브러리 설치\n",
        "# !pip install opencv-python timm torch torchvision pandas scikit-learn tqdm albumentations\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a, b])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (약한 증강 포함)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384), augment=False):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "        if augment:\n",
        "            # 약한 증강 설정\n",
        "            self.transform = A.Compose([\n",
        "                A.RandomRotate90(p=0.3),\n",
        "                A.HorizontalFlip(p=0.3),\n",
        "                A.ShiftScaleRotate(\n",
        "                    shift_limit=0.03,\n",
        "                    scale_limit=0.05,\n",
        "                    rotate_limit=8,\n",
        "                    p=0.3\n",
        "                ),\n",
        "                A.RandomBrightnessContrast(\n",
        "                    brightness_limit=0.1,\n",
        "                    contrast_limit=0.1,\n",
        "                    p=0.3\n",
        "                ),\n",
        "                A.GaussNoise(\n",
        "                    var_limit=(5.0, 25.0),\n",
        "                    p=0.3\n",
        "                ),\n",
        "                A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "                ToTensorV2(),\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = A.Compose([\n",
        "                A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "                ToTensorV2(),\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "\n",
        "        pil_img = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img_np = np.array(pil_img)\n",
        "        img_t = self.transform(image=img_np)['image']\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS 데이터 로드 및 필터링\n",
        "# ----------------------------------------\n",
        "ROOT    = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path= os.path.join(ROOT, 'train.csv')\n",
        "img_dir = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "def file_exists(fname):\n",
        "    if not fname.lower().endswith('.png'):\n",
        "        fname = f\"{fname}.png\"\n",
        "    return os.path.isfile(os.path.join(img_dir, fname))\n",
        "full_df = full_df[ full_df['image'].apply(file_exists) ].reset_index(drop=True)\n",
        "print(f\"총 {len(full_df)}개 사용\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10)\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, holdout_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train   = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_holdout = full_df.iloc[holdout_idx].reset_index(drop=True)\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_holdout, df_holdout['label']))\n",
        "df_val  = df_holdout.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_holdout.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 설정\n",
        "# ----------------------------------------\n",
        "class_counts = df_train['label'].value_counts().sort_index().tolist()\n",
        "class_weights = [1.0 / c for c in class_counts]\n",
        "sample_weights = df_train['label'].apply(lambda x: class_weights[x]).tolist()\n",
        "train_sampler = WeightedRandomSampler(\n",
        "    weights=sample_weights,\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) 데이터로더 준비\n",
        "# ----------------------------------------\n",
        "batch_size = 16\n",
        "train_ds = APTOSDataset(df_train, img_dir, augment=True)\n",
        "val_ds   = APTOSDataset(df_val,   img_dir, augment=False)\n",
        "test_ds  = APTOSDataset(df_test,  img_dir, augment=False)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, sampler=train_sampler,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False,\n",
        "                          num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 옵티마이저 · OneCycleLR · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = timm.create_model(\n",
        "    'swin_large_patch4_window12_384.ms_in22k_ft_in1k',\n",
        "    pretrained=True, num_classes=5\n",
        ").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(\n",
        "    optimizer, max_lr=1e-3,\n",
        "    steps_per_epoch=len(train_loader), epochs=NUM_EPOCHS,\n",
        "    pct_start=0.1, div_factor=25.0\n",
        ")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "num_classes  = 5\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    # Train\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    epoch_counts = [0]*num_classes\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch}'):\n",
        "        for l in labels.cpu().tolist(): epoch_counts[l] += 1\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(): out = model(imgs); loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update(); scheduler.step()\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "    print(f\"Epoch {epoch:03d} — Train Loss: {t_loss/t_total:.4f}, Acc: {t_correct/t_total:.4f}\")\n",
        "    print(\"  Epoch Sampling Counts by Class:\", epoch_counts)\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out = model(imgs); loss = criterion(out, labels)\n",
        "            v_loss    += loss.item()*imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "    all_p = torch.cat(all_p).numpy(); all_l = torch.cat(all_l).numpy()\n",
        "    print(f\"Epoch {epoch:03d} — Valid Loss: {v_loss/v_total:.4f}, Acc: {v_correct/v_total:.4f}\")\n",
        "    print(f\"  P: {precision_score(all_l, all_p, average='macro', zero_division=0):.4f}\",\"\n",
        "          f\" R: {recall_score(all_l, all_p, average='macro', zero_division=0):.4f}\",\"\n",
        "          f\" F1: {f1_score(all_l, all_p, average='macro', zero_division=0):.4f}\")\n",
        "    print(\"  Confusion Matrix:\"); print(confusion_matrix(all_l, all_p))\n",
        "    if (v_correct/v_total) > best_val_acc:\n",
        "        best_val_acc = v_correct/v_total; no_improve=0; torch.save(model.state_dict(), 'best_model.pth'); print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience: print(\"Early stopping.\"); break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model('swin_large_patch4_window12_384.ms_in22k_ft_in1k', pretrained=False, num_classes=5).to(device)\n",
        "best_model.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out = best_model(imgs); loss = criterion(out, labels)\n",
        "        t_loss    += loss.item()*imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu()); all_l.append(labels.cpu())\n",
        "all_p = torch.cat(all_p).numpy(); all_l = torch.cat(all_l).numpy()\n",
        "print(\"=== Test Performance ===\")\n",
        "print(f\"Acc: {t_correct/t_total:.4f}\",\"\n",
        "      f\" P: {precision_score(all_l, all_p, average='macro', zero_division=0):.4f}\",\"\n",
        "      f\" R: {recall_score(all_l, all_p, average='macro', zero_division=0):.4f}\",\"\n",
        "      f\" F1: {f1_score(all_l, all_p, average='macro', zero_division=0):.4f}\")\n",
        "print(\"Confusion Matrix:\"); print(confusion_matrix(all_l, all_p))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "tphZVIVJp-_7",
        "outputId": "e8a753d3-c845-4ff5-a52c-e1d741907bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 231) (<ipython-input-8-a1c371b9b97e>, line 231)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-a1c371b9b97e>\"\u001b[0;36m, line \u001b[0;32m231\u001b[0m\n\u001b[0;31m    print(f\"  P: {precision_score(all_l, all_p, average='macro', zero_division=0):.4f}\",\"\u001b[0m\n\u001b[0m                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 231)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3n2767RuBTIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''APTOS 전체 데이터, OnDAT, WeightedRamdomSampler 적용, Loss weighting (CrossEntropyLoss(weight))\n",
        "RandomRotate\n",
        "RandomBrightnessContrast\n",
        "GaussianNoise\n",
        "HueSaturationValue\n",
        "Cutout (RandomErasing)\n",
        "(참고: Albumentations 라이브러리 기반, 각 증강 강도는 안저(fundus) 영상 문헌에서 자주 쓰이는 범위로 설정)'''\n",
        "\n",
        "# 필요한 라이브러리 설치 (Colab)\n",
        "#!pip install --upgrade albumentations opencv-python timm torch torchvision pandas scikit-learn tqdm\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# ----------------------------------------\n",
        "# 1) 전처리 함수 정의 (원판 크롭 + CLAHE + 리사이즈)\n",
        "# ----------------------------------------\n",
        "def preprocess_fundus_image(img_path, output_size=(384, 384)):\n",
        "    img_bgr = cv2.imread(img_path)\n",
        "    if img_bgr is None:\n",
        "        raise ValueError(f\"이미지를 읽을 수 없습니다: {img_path}\")\n",
        "    gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))\n",
        "    binary_closed = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "    contours, _ = cv2.findContours(binary_closed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    h, w = img_bgr.shape[:2]\n",
        "\n",
        "    if not contours or cv2.contourArea(max(contours, key=cv2.contourArea)) < h*w*0.1:\n",
        "        cropped = img_bgr.copy()\n",
        "    else:\n",
        "        cnt = max(contours, key=cv2.contourArea)\n",
        "        (cx, cy), radius = cv2.minEnclosingCircle(cnt)\n",
        "        cx, cy, radius = map(int, (cx, cy, radius))\n",
        "        mask = np.zeros_like(gray)\n",
        "        cv2.circle(mask, (cx, cy), radius, 255, -1)\n",
        "        masked = cv2.bitwise_and(img_bgr, img_bgr, mask=mask)\n",
        "        x1, y1 = max(cx-radius, 0), max(cy-radius, 0)\n",
        "        x2, y2 = min(cx+radius, w), min(cy+radius, h)\n",
        "        cropped = masked[y1:y2, x1:x2]\n",
        "\n",
        "    lab = cv2.cvtColor(cropped, cv2.COLOR_BGR2LAB)\n",
        "    l, a_chan, b_chan = cv2.split(lab)\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "    l_clahe = clahe.apply(l)\n",
        "    lab_clahe = cv2.merge([l_clahe, a_chan, b_chan])\n",
        "    bgr_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "    if output_size is not None:\n",
        "        bgr_clahe = cv2.resize(bgr_clahe, output_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    img_rgb = cv2.cvtColor(bgr_clahe, cv2.COLOR_BGR2RGB)\n",
        "    return Image.fromarray(img_rgb)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2) Dataset 정의 (증강 포함)\n",
        "# ----------------------------------------\n",
        "class APTOSDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, preprocess_size=(384,384), train=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_dir = img_dir\n",
        "        self.preprocess_size = preprocess_size\n",
        "        self.train = train\n",
        "\n",
        "        if self.train:\n",
        "            self.aug = A.Compose([\n",
        "                A.RandomRotate90(p=0.5),\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n",
        "                A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.5),\n",
        "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
        "                A.CoarseDropout(min_holes=1, max_holes=8, max_height=32, max_width=32, fill_value=0, p=0.5),\n",
        "                A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "        else:\n",
        "            self.aug = A.Compose([\n",
        "                A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row   = self.df.iloc[idx]\n",
        "        fname = row['image']\n",
        "        label = int(row['label'])\n",
        "        if not fname.lower().endswith('.png'):\n",
        "            fname = f\"{fname}.png\"\n",
        "        img_path = os.path.join(self.img_dir, fname)\n",
        "        pil_img = preprocess_fundus_image(img_path, output_size=self.preprocess_size)\n",
        "        img_np  = np.array(pil_img)\n",
        "        img_t   = self.aug(image=img_np)['image']\n",
        "        return img_t, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3) APTOS train.csv 로드 및 필터링\n",
        "# ----------------------------------------\n",
        "ROOT     = '/content/drive/MyDrive/DL_Project_17/YJ/APTOS'\n",
        "csv_path = os.path.join(ROOT, 'train.csv')\n",
        "img_dir  = os.path.join(ROOT, 'train_images')\n",
        "\n",
        "full_df = pd.read_csv(csv_path).rename(columns={'diagnosis':'label'})\n",
        "full_df = full_df[ full_df['image'].apply(lambda f: os.path.isfile(os.path.join(img_dir, f if f.lower().endswith('.png') else f+'.png'))) ].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4) Stratified Split (80/10/10)\n",
        "# ----------------------------------------\n",
        "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
        "train_idx, holdout_idx = next(sss1.split(full_df, full_df['label']))\n",
        "df_train   = full_df.iloc[train_idx].reset_index(drop=True)\n",
        "df_holdout = full_df.iloc[holdout_idx].reset_index(drop=True)\n",
        "\n",
        "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "val_idx, test_idx = next(sss2.split(df_holdout, df_holdout['label']))\n",
        "df_val  = df_holdout.iloc[val_idx].reset_index(drop=True)\n",
        "df_test = df_holdout.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5) WeightedRandomSampler 준비\n",
        "# ----------------------------------------\n",
        "class_counts    = df_train['label'].value_counts().sort_index().values\n",
        "class_weights_s = 1.0 / class_counts\n",
        "sample_weights  = class_weights_s[df_train['label'].values]\n",
        "sampler         = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6) Dataset & DataLoader 준비\n",
        "# ----------------------------------------\n",
        "train_ds = APTOSDataset(df_train, img_dir, train=True)\n",
        "val_ds   = APTOSDataset(df_val,   img_dir, train=False)\n",
        "test_ds  = APTOSDataset(df_test,  img_dir, train=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=16, sampler=sampler, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(val_ds,   batch_size=16, shuffle=False,   num_workers=4, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=16, shuffle=False,   num_workers=4, pin_memory=True)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7) 모델 · 손실함수(가중치) · 옵티마이저 · LR 스케줄러 · AMP 세팅\n",
        "# ----------------------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model  = timm.create_model('swin_large_patch4_window12_384.ms_in22k_ft_in1k', pretrained=True, num_classes=5).to(device)\n",
        "\n",
        "# ---- 여기에 Loss Weighting 적용 ----\n",
        "class_weights_tensor = torch.tensor(class_weights_s, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "NUM_EPOCHS = 50\n",
        "scaler     = GradScaler()\n",
        "scheduler  = OneCycleLR(optimizer, max_lr=1e-3, steps_per_epoch=len(train_loader),\n",
        "                        epochs=NUM_EPOCHS, pct_start=0.1, div_factor=25.0)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8) Training + Validation 루프\n",
        "# ----------------------------------------\n",
        "best_val_acc = 0.0\n",
        "patience     = 5\n",
        "no_improve   = 0\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    # — Train —\n",
        "    model.train()\n",
        "    t_loss = t_correct = t_total = 0\n",
        "    for imgs, labels in tqdm(train_loader, desc=f'Train Epoch {epoch:03d}'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with autocast():\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        t_loss    += loss.item() * imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "\n",
        "    train_loss = t_loss / t_total\n",
        "    train_acc  = t_correct / t_total\n",
        "    print(f\"Epoch {epoch:03d} — Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
        "\n",
        "    # — Validate —\n",
        "    model.eval()\n",
        "    v_loss = v_correct = v_total = 0\n",
        "    all_p, all_l = [], []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(valid_loader, desc='Valid'):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            out  = model(imgs)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            v_loss    += loss.item() * imgs.size(0)\n",
        "            preds      = out.argmax(dim=1)\n",
        "            v_correct += (preds==labels).sum().item()\n",
        "            v_total   += labels.size(0)\n",
        "            all_p.append(preds.cpu())\n",
        "            all_l.append(labels.cpu())\n",
        "\n",
        "    valid_loss = v_loss / v_total\n",
        "    valid_acc  = v_correct / v_total\n",
        "    all_p = torch.cat(all_p).numpy()\n",
        "    all_l = torch.cat(all_l).numpy()\n",
        "    v_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "    v_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "    print(f\"Epoch {epoch:03d} — Valid Loss: {valid_loss:.4f}, Acc: {valid_acc:.4f}\")\n",
        "    print(f\"  P: {v_prec:.4f}, R: {v_rec:.4f}, F1: {v_f1:.4f}\")\n",
        "    print(\"  Confusion Matrix:\")\n",
        "    print(v_cm)\n",
        "\n",
        "    if valid_acc > best_val_acc:\n",
        "        best_val_acc = valid_acc\n",
        "        no_improve   = 0\n",
        "        torch.save(model.state_dict(), 'best_swin_large384_aptos.pth')\n",
        "        print(\">> Best model saved.\")\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "# ----------------------------------------\n",
        "# 9) Test Set 평가\n",
        "# ----------------------------------------\n",
        "best_model = timm.create_model('swin_large_patch4_window12_384.ms_in22k_ft_in1k', pretrained=False, num_classes=5).to(device)\n",
        "best_model.load_state_dict(torch.load('best_swin_large384_aptos.pth', map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "t_loss = t_correct = t_total = 0\n",
        "all_p, all_l = [], []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in tqdm(test_loader, desc='Test'):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        out  = best_model(imgs)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        t_loss    += loss.item() * imgs.size(0)\n",
        "        preds      = out.argmax(dim=1)\n",
        "        t_correct += (preds==labels).sum().item()\n",
        "        t_total   += labels.size(0)\n",
        "        all_p.append(preds.cpu())\n",
        "        all_l.append(labels.cpu())\n",
        "\n",
        "test_loss = t_loss / t_total\n",
        "test_acc  = t_correct / t_total\n",
        "all_p = torch.cat(all_p).numpy()\n",
        "all_l = torch.cat(all_l).numpy()\n",
        "t_prec = precision_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_rec  = recall_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_f1   = f1_score(all_l, all_p, average='macro', zero_division=0)\n",
        "t_cm   = confusion_matrix(all_l, all_p)\n",
        "\n",
        "print(\"\\n=== Test Set Performance ===\")\n",
        "print(f\"Test Loss : {test_loss:.4f}\")\n",
        "print(f\"Test Acc  : {test_acc:.4f}\")\n",
        "print(f\"Precision : {t_prec:.4f}\")\n",
        "print(f\"Recall    : {t_rec:.4f}\")\n",
        "print(f\"F1-score  : {t_f1:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(t_cm)"
      ],
      "metadata": {
        "id": "0YXe0yp0fxiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/best_swin_large384_3.pth /content/drive/MyDrive/DL_Project_17/.pth"
      ],
      "metadata": {
        "id": "tKMhURwkNgCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zcfYtld1NgPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PS94GyKcNgXv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}